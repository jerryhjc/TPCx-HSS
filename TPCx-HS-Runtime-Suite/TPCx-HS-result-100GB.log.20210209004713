
[0;32mRunning 100GB test[0m
[0;32mHSsize is 1000000000[0m
[0;32mAll Output will be logged to file ./TPCx-HS-result-100GB.log[0m

[0;31mCLUSH NOT INSTALLED for cluster audit report[0m
[0;31mTo install clush follow USER_GUIDE.txt[0m
[0;32m===================================[0m
[0;32mDeleting Previous Data - Start - Mon Feb  8 23:57:45 UTC 2021[0m
[0;32mDeleting Previous Data - End - Mon Feb  8 23:58:48 UTC 2021[0m
[0;32m===================================[0m


[0;32m===================================[0m
[0;32m Running BigData TPCx-HS Benchmark Suite (MapReduce) - Run 1 - Epoch 1612828728 [0m
[0;32m TPCx-HS Version 2.0.3 [0m
[0;32m===================================[0m

[0;32mStarting HSGen Run 1 (output being written to ./logs/HSgen-time-run1.txt)[0m

2021-02-08 23:58:49,184 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at hd1/10.0.9.11:8032
2021-02-08 23:58:49,436 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/ec2-user/.staging/job_1612828479085_0007
2021-02-08 23:58:49,610 INFO HSSort: Generating 1000000000 using 379
2021-02-08 23:58:49,662 INFO mapreduce.JobSubmitter: number of splits:379
2021-02-08 23:58:49,685 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
2021-02-08 23:58:49,686 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps
2021-02-08 23:58:49,753 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1612828479085_0007
2021-02-08 23:58:49,753 INFO mapreduce.JobSubmitter: Executing with tokens: []
2021-02-08 23:58:49,884 INFO conf.Configuration: resource-types.xml not found
2021-02-08 23:58:49,884 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.
2021-02-08 23:58:49,932 INFO impl.YarnClientImpl: Submitted application application_1612828479085_0007
2021-02-08 23:58:49,963 INFO mapreduce.Job: The url to track the job: http://hd1:8088/proxy/application_1612828479085_0007/
2021-02-08 23:58:49,963 INFO mapreduce.Job: Running job: job_1612828479085_0007
2021-02-08 23:58:55,021 INFO mapreduce.Job: Job job_1612828479085_0007 running in uber mode : false
2021-02-08 23:58:55,022 INFO mapreduce.Job:  map 0% reduce 0%
2021-02-08 23:59:01,104 INFO mapreduce.Job:  map 1% reduce 0%
2021-02-08 23:59:03,146 INFO mapreduce.Job:  map 2% reduce 0%
2021-02-08 23:59:06,176 INFO mapreduce.Job:  map 3% reduce 0%
2021-02-08 23:59:07,180 INFO mapreduce.Job:  map 4% reduce 0%
2021-02-08 23:59:08,192 INFO mapreduce.Job:  map 5% reduce 0%
2021-02-08 23:59:09,197 INFO mapreduce.Job:  map 6% reduce 0%
2021-02-08 23:59:11,242 INFO mapreduce.Job:  map 7% reduce 0%
2021-02-08 23:59:13,303 INFO mapreduce.Job:  map 8% reduce 0%
2021-02-08 23:59:16,344 INFO mapreduce.Job:  map 9% reduce 0%
2021-02-08 23:59:17,359 INFO mapreduce.Job:  map 10% reduce 0%
2021-02-08 23:59:18,376 INFO mapreduce.Job:  map 11% reduce 0%
2021-02-08 23:59:19,397 INFO mapreduce.Job:  map 12% reduce 0%
2021-02-08 23:59:21,415 INFO mapreduce.Job:  map 13% reduce 0%
2021-02-08 23:59:22,433 INFO mapreduce.Job:  map 14% reduce 0%
2021-02-08 23:59:23,471 INFO mapreduce.Job:  map 15% reduce 0%
2021-02-08 23:59:25,515 INFO mapreduce.Job:  map 16% reduce 0%
2021-02-08 23:59:29,553 INFO mapreduce.Job:  map 17% reduce 0%
2021-02-08 23:59:31,563 INFO mapreduce.Job:  map 18% reduce 0%
2021-02-08 23:59:34,582 INFO mapreduce.Job:  map 19% reduce 0%
2021-02-08 23:59:36,588 INFO mapreduce.Job:  map 20% reduce 0%
2021-02-08 23:59:39,598 INFO mapreduce.Job:  map 21% reduce 0%
2021-02-08 23:59:41,604 INFO mapreduce.Job:  map 22% reduce 0%
2021-02-08 23:59:44,614 INFO mapreduce.Job:  map 23% reduce 0%
2021-02-08 23:59:47,623 INFO mapreduce.Job:  map 24% reduce 0%
2021-02-08 23:59:50,633 INFO mapreduce.Job:  map 25% reduce 0%
2021-02-08 23:59:54,645 INFO mapreduce.Job:  map 26% reduce 0%
2021-02-08 23:59:57,655 INFO mapreduce.Job:  map 27% reduce 0%
2021-02-09 00:00:00,665 INFO mapreduce.Job:  map 28% reduce 0%
2021-02-09 00:00:03,675 INFO mapreduce.Job:  map 29% reduce 0%
2021-02-09 00:00:06,685 INFO mapreduce.Job:  map 30% reduce 0%
2021-02-09 00:00:09,694 INFO mapreduce.Job:  map 31% reduce 0%
2021-02-09 00:00:11,701 INFO mapreduce.Job:  map 32% reduce 0%
2021-02-09 00:00:14,711 INFO mapreduce.Job:  map 33% reduce 0%
2021-02-09 00:00:16,717 INFO mapreduce.Job:  map 34% reduce 0%
2021-02-09 00:00:19,727 INFO mapreduce.Job:  map 35% reduce 0%
2021-02-09 00:00:22,737 INFO mapreduce.Job:  map 36% reduce 0%
2021-02-09 00:00:24,743 INFO mapreduce.Job:  map 37% reduce 0%
2021-02-09 00:00:26,750 INFO mapreduce.Job:  map 38% reduce 0%
2021-02-09 00:00:29,759 INFO mapreduce.Job:  map 39% reduce 0%
2021-02-09 00:00:32,799 INFO mapreduce.Job:  map 40% reduce 0%
2021-02-09 00:00:34,816 INFO mapreduce.Job:  map 41% reduce 0%
2021-02-09 00:00:37,894 INFO mapreduce.Job:  map 42% reduce 0%
2021-02-09 00:00:40,936 INFO mapreduce.Job:  map 43% reduce 0%
2021-02-09 00:00:41,939 INFO mapreduce.Job:  map 44% reduce 0%
2021-02-09 00:00:45,951 INFO mapreduce.Job:  map 45% reduce 0%
2021-02-09 00:00:46,955 INFO mapreduce.Job:  map 46% reduce 0%
2021-02-09 00:00:47,958 INFO mapreduce.Job:  map 47% reduce 0%
2021-02-09 00:00:51,971 INFO mapreduce.Job:  map 48% reduce 0%
2021-02-09 00:00:52,974 INFO mapreduce.Job:  map 49% reduce 0%
2021-02-09 00:00:54,984 INFO mapreduce.Job:  map 50% reduce 0%
2021-02-09 00:00:58,996 INFO mapreduce.Job:  map 51% reduce 0%
2021-02-09 00:01:02,005 INFO mapreduce.Job:  map 52% reduce 0%
2021-02-09 00:01:05,014 INFO mapreduce.Job:  map 53% reduce 0%
2021-02-09 00:01:08,022 INFO mapreduce.Job:  map 54% reduce 0%
2021-02-09 00:01:11,032 INFO mapreduce.Job:  map 55% reduce 0%
2021-02-09 00:01:13,038 INFO mapreduce.Job:  map 56% reduce 0%
2021-02-09 00:01:17,049 INFO mapreduce.Job:  map 57% reduce 0%
2021-02-09 00:01:19,064 INFO mapreduce.Job:  map 58% reduce 0%
2021-02-09 00:01:22,080 INFO mapreduce.Job:  map 59% reduce 0%
2021-02-09 00:01:25,089 INFO mapreduce.Job:  map 60% reduce 0%
2021-02-09 00:01:27,107 INFO mapreduce.Job:  map 61% reduce 0%
2021-02-09 00:01:30,117 INFO mapreduce.Job:  map 62% reduce 0%
2021-02-09 00:01:32,123 INFO mapreduce.Job:  map 63% reduce 0%
2021-02-09 00:01:35,136 INFO mapreduce.Job:  map 64% reduce 0%
2021-02-09 00:01:37,151 INFO mapreduce.Job:  map 65% reduce 0%
2021-02-09 00:01:40,181 INFO mapreduce.Job:  map 66% reduce 0%
2021-02-09 00:01:43,207 INFO mapreduce.Job:  map 67% reduce 0%
2021-02-09 00:01:45,230 INFO mapreduce.Job:  map 68% reduce 0%
2021-02-09 00:01:47,271 INFO mapreduce.Job:  map 69% reduce 0%
2021-02-09 00:01:50,286 INFO mapreduce.Job:  map 70% reduce 0%
2021-02-09 00:01:52,293 INFO mapreduce.Job:  map 71% reduce 0%
2021-02-09 00:01:55,303 INFO mapreduce.Job:  map 72% reduce 0%
2021-02-09 00:01:57,309 INFO mapreduce.Job:  map 73% reduce 0%
2021-02-09 00:01:59,314 INFO mapreduce.Job:  map 74% reduce 0%
2021-02-09 00:02:01,319 INFO mapreduce.Job:  map 75% reduce 0%
2021-02-09 00:02:04,328 INFO mapreduce.Job:  map 76% reduce 0%
2021-02-09 00:02:07,342 INFO mapreduce.Job:  map 77% reduce 0%
2021-02-09 00:02:09,350 INFO mapreduce.Job:  map 78% reduce 0%
2021-02-09 00:02:11,356 INFO mapreduce.Job:  map 79% reduce 0%
2021-02-09 00:02:14,374 INFO mapreduce.Job:  map 80% reduce 0%
2021-02-09 00:02:17,381 INFO mapreduce.Job:  map 81% reduce 0%
2021-02-09 00:02:20,390 INFO mapreduce.Job:  map 82% reduce 0%
2021-02-09 00:02:22,412 INFO mapreduce.Job:  map 83% reduce 0%
2021-02-09 00:02:23,415 INFO mapreduce.Job:  map 84% reduce 0%
2021-02-09 00:02:27,427 INFO mapreduce.Job:  map 85% reduce 0%
2021-02-09 00:02:29,448 INFO mapreduce.Job:  map 86% reduce 0%
2021-02-09 00:02:32,469 INFO mapreduce.Job:  map 87% reduce 0%
2021-02-09 00:02:34,474 INFO mapreduce.Job:  map 88% reduce 0%
2021-02-09 00:02:36,479 INFO mapreduce.Job:  map 89% reduce 0%
2021-02-09 00:02:39,487 INFO mapreduce.Job:  map 90% reduce 0%
2021-02-09 00:02:41,492 INFO mapreduce.Job:  map 91% reduce 0%
2021-02-09 00:02:43,497 INFO mapreduce.Job:  map 92% reduce 0%
2021-02-09 00:02:45,502 INFO mapreduce.Job:  map 93% reduce 0%
2021-02-09 00:02:47,506 INFO mapreduce.Job:  map 94% reduce 0%
2021-02-09 00:02:49,511 INFO mapreduce.Job:  map 95% reduce 0%
2021-02-09 00:02:52,519 INFO mapreduce.Job:  map 96% reduce 0%
2021-02-09 00:02:54,524 INFO mapreduce.Job:  map 97% reduce 0%
2021-02-09 00:02:56,529 INFO mapreduce.Job:  map 98% reduce 0%
2021-02-09 00:02:59,536 INFO mapreduce.Job:  map 99% reduce 0%
2021-02-09 00:03:01,541 INFO mapreduce.Job:  map 100% reduce 0%
2021-02-09 00:03:02,547 INFO mapreduce.Job: Job job_1612828479085_0007 completed successfully
2021-02-09 00:03:02,618 INFO mapreduce.Job: Counters: 35
	File System Counters
		FILE: Number of bytes read=0
		FILE: Number of bytes written=100118046
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=18182
		HDFS: Number of bytes written=100000000000
		HDFS: Number of read operations=2274
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=758
		HDFS: Number of bytes read erasure-coded=0
	Job Counters 
		Killed map tasks=1
		Launched map tasks=380
		Other local map tasks=380
		Total time spent by all maps in occupied slots (ms)=17367828
		Total time spent by all reduces in occupied slots (ms)=0
		Total time spent by all map tasks (ms)=17367828
		Total vcore-milliseconds taken by all map tasks=17367828
		Total megabyte-milliseconds taken by all map tasks=17784655872
	Map-Reduce Framework
		Map input records=1000000000
		Map output records=1000000000
		Input split bytes=18182
		Spilled Records=0
		Failed Shuffles=0
		Merged Map outputs=0
		GC time elapsed (ms)=62754
		CPU time spent (ms)=1439790
		Physical memory (bytes) snapshot=146239062016
		Virtual memory (bytes) snapshot=956410888192
		Total committed heap usage (bytes)=163608264704
		Peak Map Physical memory (bytes)=417660928
		Peak Map Virtual memory (bytes)=2531852288
	HSGen$Counters
		CHECKSUM=2147523228284173905
	File Input Format Counters 
		Bytes Read=0
	File Output Format Counters 
		Bytes Written=100000000000

real	4m14.918s
user	0m4.890s
sys	0m0.137s


[0;32m======== HSgen Result SUCCESS ========[0m
[0;32m======== Time taken by HSGen = 4m14.918s====[0m



[0;32mListing HSGen output [0m

-rw-r--r--   2 ec2-user ec2-user          0 2021-02-09 00:03 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/_SUCCESS
-rw-r--r--   2 ec2-user ec2-user  263852300 2021-02-08 23:58 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00000
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-08 23:58 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00001
-rw-r--r--   2 ec2-user ec2-user  263852300 2021-02-08 23:58 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00002
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-08 23:59 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00003
-rw-r--r--   2 ec2-user ec2-user  263852300 2021-02-08 23:59 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00004
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-08 23:59 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00005
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-08 23:59 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00006
-rw-r--r--   2 ec2-user ec2-user  263852300 2021-02-08 23:59 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00007
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-08 23:59 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00008
-rw-r--r--   2 ec2-user ec2-user  263852300 2021-02-08 23:59 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00009
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-08 23:59 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00010
-rw-r--r--   2 ec2-user ec2-user  263852300 2021-02-08 23:59 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00011
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-08 23:59 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00012
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-08 23:59 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00013
-rw-r--r--   2 ec2-user ec2-user  263852300 2021-02-08 23:59 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00014
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-08 23:59 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00015
-rw-r--r--   2 ec2-user ec2-user  263852300 2021-02-08 23:59 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00016
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-08 23:59 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00017
-rw-r--r--   2 ec2-user ec2-user  263852300 2021-02-08 23:59 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00018
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-08 23:59 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00019
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-08 23:59 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00020
-rw-r--r--   2 ec2-user ec2-user  263852300 2021-02-08 23:59 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00021
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-08 23:59 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00022
-rw-r--r--   2 ec2-user ec2-user  263852300 2021-02-08 23:59 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00023
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-08 23:59 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00024
-rw-r--r--   2 ec2-user ec2-user  263852300 2021-02-08 23:59 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00025
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-08 23:59 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00026
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-08 23:59 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00027
-rw-r--r--   2 ec2-user ec2-user  263852300 2021-02-08 23:59 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00028
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-08 23:59 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00029
-rw-r--r--   2 ec2-user ec2-user  263852300 2021-02-08 23:59 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00030
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-08 23:59 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00031
-rw-r--r--   2 ec2-user ec2-user  263852300 2021-02-08 23:59 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00032
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-08 23:59 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00033
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-08 23:59 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00034
-rw-r--r--   2 ec2-user ec2-user  263852300 2021-02-08 23:59 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00035
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-08 23:59 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00036
-rw-r--r--   2 ec2-user ec2-user  263852300 2021-02-08 23:59 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00037
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-08 23:59 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00038
-rw-r--r--   2 ec2-user ec2-user  263852300 2021-02-08 23:59 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00039
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-08 23:59 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00040
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-08 23:59 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00041
-rw-r--r--   2 ec2-user ec2-user  263852300 2021-02-08 23:59 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00042
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-08 23:59 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00043
-rw-r--r--   2 ec2-user ec2-user  263852300 2021-02-08 23:59 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00044
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-08 23:59 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00045
-rw-r--r--   2 ec2-user ec2-user  263852300 2021-02-08 23:59 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00046
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-08 23:59 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00047
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-08 23:59 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00048
-rw-r--r--   2 ec2-user ec2-user  263852300 2021-02-08 23:59 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00049
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-08 23:59 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00050
-rw-r--r--   2 ec2-user ec2-user  263852300 2021-02-08 23:59 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00051
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-08 23:59 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00052
-rw-r--r--   2 ec2-user ec2-user  263852300 2021-02-08 23:59 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00053
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-08 23:59 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00054
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-08 23:59 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00055
-rw-r--r--   2 ec2-user ec2-user  263852300 2021-02-08 23:59 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00056
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-08 23:59 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00057
-rw-r--r--   2 ec2-user ec2-user  263852300 2021-02-08 23:59 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00058
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-08 23:59 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00059
-rw-r--r--   2 ec2-user ec2-user  263852300 2021-02-08 23:59 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00060
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-08 23:59 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00061
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-08 23:59 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00062
-rw-r--r--   2 ec2-user ec2-user  263852300 2021-02-08 23:59 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00063
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-08 23:59 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00064
-rw-r--r--   2 ec2-user ec2-user  263852300 2021-02-09 00:00 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00065
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-08 23:59 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00066
-rw-r--r--   2 ec2-user ec2-user  263852300 2021-02-08 23:59 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00067
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-09 00:00 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00068
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-09 00:00 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00069
-rw-r--r--   2 ec2-user ec2-user  263852300 2021-02-09 00:00 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00070
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-09 00:00 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00071
-rw-r--r--   2 ec2-user ec2-user  263852300 2021-02-09 00:00 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00072
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-09 00:00 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00073
-rw-r--r--   2 ec2-user ec2-user  263852300 2021-02-09 00:00 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00074
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-09 00:00 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00075
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-09 00:00 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00076
-rw-r--r--   2 ec2-user ec2-user  263852300 2021-02-09 00:00 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00077
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-09 00:00 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00078
-rw-r--r--   2 ec2-user ec2-user  263852300 2021-02-09 00:00 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00079
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-09 00:00 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00080
-rw-r--r--   2 ec2-user ec2-user  263852300 2021-02-09 00:00 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00081
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-09 00:00 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00082
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-08 23:59 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00083
-rw-r--r--   2 ec2-user ec2-user  263852300 2021-02-09 00:00 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00084
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-09 00:00 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00085
-rw-r--r--   2 ec2-user ec2-user  263852300 2021-02-09 00:00 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00086
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-09 00:00 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00087
-rw-r--r--   2 ec2-user ec2-user  263852300 2021-02-09 00:00 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00088
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-09 00:00 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00089
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-09 00:00 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00090
-rw-r--r--   2 ec2-user ec2-user  263852300 2021-02-09 00:00 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00091
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-09 00:00 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00092
-rw-r--r--   2 ec2-user ec2-user  263852300 2021-02-09 00:00 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00093
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-09 00:00 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00094
-rw-r--r--   2 ec2-user ec2-user  263852300 2021-02-09 00:00 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00095
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-09 00:00 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00096
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-09 00:00 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00097
-rw-r--r--   2 ec2-user ec2-user  263852300 2021-02-09 00:00 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00098
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-09 00:00 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00099
-rw-r--r--   2 ec2-user ec2-user  263852300 2021-02-09 00:00 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00100
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-09 00:00 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00101
-rw-r--r--   2 ec2-user ec2-user  263852300 2021-02-09 00:00 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00102
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-09 00:00 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00103
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-09 00:00 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00104
-rw-r--r--   2 ec2-user ec2-user  263852300 2021-02-09 00:00 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00105
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-09 00:00 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00106
-rw-r--r--   2 ec2-user ec2-user  263852300 2021-02-09 00:00 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00107
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-09 00:00 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00108
-rw-r--r--   2 ec2-user ec2-user  263852300 2021-02-09 00:00 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00109
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-09 00:00 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00110
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-09 00:00 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00111
-rw-r--r--   2 ec2-user ec2-user  263852300 2021-02-09 00:00 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00112
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-09 00:00 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00113
-rw-r--r--   2 ec2-user ec2-user  263852300 2021-02-09 00:00 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00114
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-09 00:00 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00115
-rw-r--r--   2 ec2-user ec2-user  263852300 2021-02-09 00:00 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00116
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-09 00:00 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00117
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-09 00:00 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00118
-rw-r--r--   2 ec2-user ec2-user  263852300 2021-02-09 00:00 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00119
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-09 00:00 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00120
-rw-r--r--   2 ec2-user ec2-user  263852300 2021-02-09 00:00 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00121
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-09 00:00 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00122
-rw-r--r--   2 ec2-user ec2-user  263852300 2021-02-09 00:00 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00123
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-09 00:00 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00124
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-09 00:00 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00125
-rw-r--r--   2 ec2-user ec2-user  263852300 2021-02-09 00:00 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00126
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-09 00:00 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00127
-rw-r--r--   2 ec2-user ec2-user  263852300 2021-02-09 00:00 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00128
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-09 00:00 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00129
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-09 00:00 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00130
-rw-r--r--   2 ec2-user ec2-user  263852300 2021-02-09 00:00 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00131
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-09 00:00 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00132
-rw-r--r--   2 ec2-user ec2-user  263852300 2021-02-09 00:00 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00133
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-09 00:00 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00134
-rw-r--r--   2 ec2-user ec2-user  263852300 2021-02-09 00:00 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00135
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-09 00:00 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00136
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-09 00:00 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00137
-rw-r--r--   2 ec2-user ec2-user  263852300 2021-02-09 00:00 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00138
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-09 00:00 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00139
-rw-r--r--   2 ec2-user ec2-user  263852300 2021-02-09 00:00 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00140
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-09 00:00 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00141
-rw-r--r--   2 ec2-user ec2-user  263852300 2021-02-09 00:00 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00142
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-09 00:00 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00143
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-09 00:00 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00144
-rw-r--r--   2 ec2-user ec2-user  263852300 2021-02-09 00:00 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00145
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-09 00:00 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00146
-rw-r--r--   2 ec2-user ec2-user  263852300 2021-02-09 00:00 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00147
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-09 00:00 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00148
-rw-r--r--   2 ec2-user ec2-user  263852300 2021-02-09 00:00 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00149
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-09 00:00 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00150
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-09 00:00 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00151
-rw-r--r--   2 ec2-user ec2-user  263852300 2021-02-09 00:01 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00152
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-09 00:01 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00153
-rw-r--r--   2 ec2-user ec2-user  263852300 2021-02-09 00:01 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00154
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-09 00:01 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00155
-rw-r--r--   2 ec2-user ec2-user  263852300 2021-02-09 00:00 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00156
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-09 00:00 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00157
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-09 00:00 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00158
-rw-r--r--   2 ec2-user ec2-user  263852300 2021-02-09 00:00 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00159
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-09 00:01 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00160
-rw-r--r--   2 ec2-user ec2-user  263852300 2021-02-09 00:01 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00161
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-09 00:01 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00162
-rw-r--r--   2 ec2-user ec2-user  263852300 2021-02-09 00:01 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00163
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-09 00:01 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00164
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-09 00:01 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00165
-rw-r--r--   2 ec2-user ec2-user  263852300 2021-02-09 00:01 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00166
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-09 00:01 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00167
-rw-r--r--   2 ec2-user ec2-user  263852300 2021-02-09 00:01 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00168
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-09 00:01 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00169
-rw-r--r--   2 ec2-user ec2-user  263852300 2021-02-09 00:01 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00170
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-09 00:01 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00171
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-09 00:01 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00172
-rw-r--r--   2 ec2-user ec2-user  263852300 2021-02-09 00:01 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00173
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-09 00:01 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00174
-rw-r--r--   2 ec2-user ec2-user  263852300 2021-02-09 00:01 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00175
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-09 00:01 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00176
-rw-r--r--   2 ec2-user ec2-user  263852300 2021-02-09 00:01 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00177
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-09 00:01 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00178
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-09 00:01 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00179
-rw-r--r--   2 ec2-user ec2-user  263852300 2021-02-09 00:01 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00180
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-09 00:01 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00181
-rw-r--r--   2 ec2-user ec2-user  263852300 2021-02-09 00:01 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00182
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-09 00:01 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00183
-rw-r--r--   2 ec2-user ec2-user  263852300 2021-02-09 00:01 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00184
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-09 00:01 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00185
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-09 00:01 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00186
-rw-r--r--   2 ec2-user ec2-user  263852300 2021-02-09 00:01 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00187
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-09 00:01 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00188
-rw-r--r--   2 ec2-user ec2-user  263852300 2021-02-09 00:01 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00189
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-09 00:01 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00190
-rw-r--r--   2 ec2-user ec2-user  263852300 2021-02-09 00:01 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00191
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-09 00:01 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00192
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-09 00:01 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00193
-rw-r--r--   2 ec2-user ec2-user  263852300 2021-02-09 00:01 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00194
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-09 00:01 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00195
-rw-r--r--   2 ec2-user ec2-user  263852300 2021-02-09 00:01 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00196
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-09 00:01 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00197
-rw-r--r--   2 ec2-user ec2-user  263852300 2021-02-09 00:01 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00198
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-09 00:01 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00199
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-09 00:01 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00200
-rw-r--r--   2 ec2-user ec2-user  263852300 2021-02-09 00:01 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00201
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-09 00:01 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00202
-rw-r--r--   2 ec2-user ec2-user  263852300 2021-02-09 00:01 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00203
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-09 00:01 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00204
-rw-r--r--   2 ec2-user ec2-user  263852300 2021-02-09 00:01 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00205
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-09 00:01 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00206
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-09 00:01 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00207
-rw-r--r--   2 ec2-user ec2-user  263852300 2021-02-09 00:01 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00208
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-09 00:01 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00209
-rw-r--r--   2 ec2-user ec2-user  263852300 2021-02-09 00:01 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00210
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-09 00:01 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00211
-rw-r--r--   2 ec2-user ec2-user  263852300 2021-02-09 00:01 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00212
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-09 00:01 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00213
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-09 00:01 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00214
-rw-r--r--   2 ec2-user ec2-user  263852300 2021-02-09 00:01 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00215
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-09 00:01 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00216
-rw-r--r--   2 ec2-user ec2-user  263852300 2021-02-09 00:01 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00217
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-09 00:01 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00218
-rw-r--r--   2 ec2-user ec2-user  263852300 2021-02-09 00:01 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00219
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-09 00:01 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00220
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-09 00:01 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00221
-rw-r--r--   2 ec2-user ec2-user  263852300 2021-02-09 00:01 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00222
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-09 00:01 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00223
-rw-r--r--   2 ec2-user ec2-user  263852300 2021-02-09 00:01 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00224
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-09 00:01 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00225
-rw-r--r--   2 ec2-user ec2-user  263852300 2021-02-09 00:01 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00226
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-09 00:01 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00227
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-09 00:01 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00228
-rw-r--r--   2 ec2-user ec2-user  263852300 2021-02-09 00:01 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00229
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-09 00:01 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00230
-rw-r--r--   2 ec2-user ec2-user  263852300 2021-02-09 00:01 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00231
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-09 00:01 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00232
-rw-r--r--   2 ec2-user ec2-user  263852300 2021-02-09 00:01 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00233
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-09 00:01 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00234
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-09 00:01 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00235
-rw-r--r--   2 ec2-user ec2-user  263852300 2021-02-09 00:01 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00236
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-09 00:01 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00237
-rw-r--r--   2 ec2-user ec2-user  263852300 2021-02-09 00:01 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00238
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-09 00:01 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00239
-rw-r--r--   2 ec2-user ec2-user  263852300 2021-02-09 00:01 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00240
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-09 00:01 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00241
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-09 00:02 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00242
-rw-r--r--   2 ec2-user ec2-user  263852300 2021-02-09 00:01 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00243
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-09 00:02 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00244
-rw-r--r--   2 ec2-user ec2-user  263852300 2021-02-09 00:02 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00245
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-09 00:02 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00246
-rw-r--r--   2 ec2-user ec2-user  263852300 2021-02-09 00:02 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00247
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-09 00:02 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00248
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-09 00:02 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00249
-rw-r--r--   2 ec2-user ec2-user  263852300 2021-02-09 00:02 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00250
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-09 00:02 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00251
-rw-r--r--   2 ec2-user ec2-user  263852300 2021-02-09 00:02 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00252
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-09 00:01 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00253
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-09 00:02 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00254
-rw-r--r--   2 ec2-user ec2-user  263852300 2021-02-09 00:01 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00255
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-09 00:02 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00256
-rw-r--r--   2 ec2-user ec2-user  263852300 2021-02-09 00:02 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00257
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-09 00:02 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00258
-rw-r--r--   2 ec2-user ec2-user  263852300 2021-02-09 00:02 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00259
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-09 00:02 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00260
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-09 00:02 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00261
-rw-r--r--   2 ec2-user ec2-user  263852300 2021-02-09 00:02 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00262
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-09 00:02 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00263
-rw-r--r--   2 ec2-user ec2-user  263852300 2021-02-09 00:02 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00264
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-09 00:02 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00265
-rw-r--r--   2 ec2-user ec2-user  263852300 2021-02-09 00:02 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00266
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-09 00:02 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00267
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-09 00:02 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00268
-rw-r--r--   2 ec2-user ec2-user  263852300 2021-02-09 00:02 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00269
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-09 00:02 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00270
-rw-r--r--   2 ec2-user ec2-user  263852300 2021-02-09 00:02 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00271
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-09 00:02 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00272
-rw-r--r--   2 ec2-user ec2-user  263852300 2021-02-09 00:02 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00273
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-09 00:02 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00274
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-09 00:02 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00275
-rw-r--r--   2 ec2-user ec2-user  263852300 2021-02-09 00:02 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00276
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-09 00:02 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00277
-rw-r--r--   2 ec2-user ec2-user  263852300 2021-02-09 00:02 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00278
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-09 00:02 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00279
-rw-r--r--   2 ec2-user ec2-user  263852300 2021-02-09 00:02 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00280
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-09 00:02 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00281
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-09 00:02 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00282
-rw-r--r--   2 ec2-user ec2-user  263852300 2021-02-09 00:02 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00283
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-09 00:02 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00284
-rw-r--r--   2 ec2-user ec2-user  263852300 2021-02-09 00:02 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00285
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-09 00:02 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00286
-rw-r--r--   2 ec2-user ec2-user  263852300 2021-02-09 00:02 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00287
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-09 00:02 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00288
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-09 00:02 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00289
-rw-r--r--   2 ec2-user ec2-user  263852300 2021-02-09 00:02 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00290
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-09 00:02 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00291
-rw-r--r--   2 ec2-user ec2-user  263852300 2021-02-09 00:02 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00292
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-09 00:02 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00293
-rw-r--r--   2 ec2-user ec2-user  263852300 2021-02-09 00:02 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00294
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-09 00:02 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00295
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-09 00:02 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00296
-rw-r--r--   2 ec2-user ec2-user  263852300 2021-02-09 00:02 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00297
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-09 00:02 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00298
-rw-r--r--   2 ec2-user ec2-user  263852300 2021-02-09 00:02 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00299
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-09 00:02 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00300
-rw-r--r--   2 ec2-user ec2-user  263852300 2021-02-09 00:02 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00301
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-09 00:02 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00302
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-09 00:02 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00303
-rw-r--r--   2 ec2-user ec2-user  263852300 2021-02-09 00:02 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00304
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-09 00:02 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00305
-rw-r--r--   2 ec2-user ec2-user  263852300 2021-02-09 00:02 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00306
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-09 00:02 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00307
-rw-r--r--   2 ec2-user ec2-user  263852300 2021-02-09 00:02 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00308
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-09 00:02 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00309
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-09 00:02 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00310
-rw-r--r--   2 ec2-user ec2-user  263852300 2021-02-09 00:02 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00311
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-09 00:02 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00312
-rw-r--r--   2 ec2-user ec2-user  263852300 2021-02-09 00:02 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00313
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-09 00:02 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00314
-rw-r--r--   2 ec2-user ec2-user  263852300 2021-02-09 00:02 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00315
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-09 00:02 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00316
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-09 00:02 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00317
-rw-r--r--   2 ec2-user ec2-user  263852300 2021-02-09 00:02 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00318
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-09 00:02 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00319
-rw-r--r--   2 ec2-user ec2-user  263852300 2021-02-09 00:02 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00320
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-09 00:02 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00321
-rw-r--r--   2 ec2-user ec2-user  263852300 2021-02-09 00:02 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00322
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-09 00:02 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00323
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-09 00:02 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00324
-rw-r--r--   2 ec2-user ec2-user  263852300 2021-02-09 00:02 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00325
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-09 00:02 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00326
-rw-r--r--   2 ec2-user ec2-user  263852300 2021-02-09 00:02 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00327
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-09 00:02 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00328
-rw-r--r--   2 ec2-user ec2-user  263852300 2021-02-09 00:02 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00329
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-09 00:02 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00330
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-09 00:02 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00331
-rw-r--r--   2 ec2-user ec2-user  263852300 2021-02-09 00:02 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00332
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-09 00:02 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00333
-rw-r--r--   2 ec2-user ec2-user  263852300 2021-02-09 00:02 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00334
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-09 00:02 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00335
-rw-r--r--   2 ec2-user ec2-user  263852300 2021-02-09 00:02 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00336
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-09 00:02 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00337
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-09 00:02 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00338
-rw-r--r--   2 ec2-user ec2-user  263852300 2021-02-09 00:02 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00339
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-09 00:02 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00340
-rw-r--r--   2 ec2-user ec2-user  263852300 2021-02-09 00:02 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00341
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-09 00:02 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00342
-rw-r--r--   2 ec2-user ec2-user  263852300 2021-02-09 00:02 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00343
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-09 00:02 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00344
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-09 00:02 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00345
-rw-r--r--   2 ec2-user ec2-user  263852300 2021-02-09 00:02 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00346
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-09 00:02 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00347
-rw-r--r--   2 ec2-user ec2-user  263852300 2021-02-09 00:02 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00348
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-09 00:02 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00349
-rw-r--r--   2 ec2-user ec2-user  263852300 2021-02-09 00:02 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00350
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-09 00:02 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00351
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-09 00:02 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00352
-rw-r--r--   2 ec2-user ec2-user  263852300 2021-02-09 00:02 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00353
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-09 00:02 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00354
-rw-r--r--   2 ec2-user ec2-user  263852300 2021-02-09 00:02 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00355
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-09 00:02 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00356
-rw-r--r--   2 ec2-user ec2-user  263852300 2021-02-09 00:02 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00357
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-09 00:02 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00358
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-09 00:02 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00359
-rw-r--r--   2 ec2-user ec2-user  263852300 2021-02-09 00:02 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00360
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-09 00:02 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00361
-rw-r--r--   2 ec2-user ec2-user  263852300 2021-02-09 00:03 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00362
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-09 00:02 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00363
-rw-r--r--   2 ec2-user ec2-user  263852300 2021-02-09 00:02 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00364
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-09 00:02 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00365
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-09 00:02 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00366
-rw-r--r--   2 ec2-user ec2-user  263852300 2021-02-09 00:03 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00367
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-09 00:03 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00368
-rw-r--r--   2 ec2-user ec2-user  263852300 2021-02-09 00:02 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00369
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-09 00:03 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00370
-rw-r--r--   2 ec2-user ec2-user  263852300 2021-02-09 00:03 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00371
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-09 00:03 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00372
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-09 00:03 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00373
-rw-r--r--   2 ec2-user ec2-user  263852300 2021-02-09 00:03 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00374
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-09 00:03 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00375
-rw-r--r--   2 ec2-user ec2-user  263852300 2021-02-09 00:03 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00376
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-09 00:02 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00377
-rw-r--r--   2 ec2-user ec2-user  263852200 2021-02-09 00:02 /user/ec2-user/TPCx-HS-benchmark/HSsort-input/part-m-00378



[0;32mStarting HSSort Run 1 (output being written to ./logs/HSsort-time-run1.txt)[0m

2021-02-09 00:03:05,008 INFO HSSort: starting
2021-02-09 00:03:05,836 INFO input.FileInputFormat: Total input files to process : 379
2021-02-09 00:03:06,244 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at hd1/10.0.9.11:8032
2021-02-09 00:03:06,453 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/ec2-user/.staging/job_1612828479085_0009
2021-02-09 00:03:06,528 INFO mapreduce.JobSubmitter: number of splits:758
2021-02-09 00:03:06,551 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
2021-02-09 00:03:06,552 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps
2021-02-09 00:03:06,617 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1612828479085_0009
2021-02-09 00:03:06,617 INFO mapreduce.JobSubmitter: Executing with tokens: []
2021-02-09 00:03:06,742 INFO conf.Configuration: resource-types.xml not found
2021-02-09 00:03:06,743 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.
2021-02-09 00:03:06,790 INFO impl.YarnClientImpl: Submitted application application_1612828479085_0009
2021-02-09 00:03:06,818 INFO mapreduce.Job: The url to track the job: http://hd1:8088/proxy/application_1612828479085_0009/
2021-02-09 00:03:06,818 INFO mapreduce.Job: Running job: job_1612828479085_0009
2021-02-09 00:03:11,879 INFO mapreduce.Job: Job job_1612828479085_0009 running in uber mode : false
2021-02-09 00:03:11,880 INFO mapreduce.Job:  map 0% reduce 0%
2021-02-09 00:03:36,311 INFO mapreduce.Job:  map 1% reduce 0%
2021-02-09 00:03:37,326 INFO mapreduce.Job:  map 3% reduce 0%
2021-02-09 00:03:38,364 INFO mapreduce.Job:  map 6% reduce 0%
2021-02-09 00:03:39,408 INFO mapreduce.Job:  map 7% reduce 0%
2021-02-09 00:03:42,430 INFO mapreduce.Job:  map 8% reduce 0%
2021-02-09 00:03:43,446 INFO mapreduce.Job:  map 9% reduce 0%
2021-02-09 00:03:44,471 INFO mapreduce.Job:  map 10% reduce 0%
2021-02-09 00:03:45,595 INFO mapreduce.Job:  map 11% reduce 0%
2021-02-09 00:04:02,891 INFO mapreduce.Job:  map 12% reduce 0%
2021-02-09 00:04:04,923 INFO mapreduce.Job:  map 13% reduce 0%
2021-02-09 00:04:06,955 INFO mapreduce.Job:  map 14% reduce 0%
2021-02-09 00:04:07,966 INFO mapreduce.Job:  map 15% reduce 0%
2021-02-09 00:04:08,977 INFO mapreduce.Job:  map 16% reduce 0%
2021-02-09 00:04:09,981 INFO mapreduce.Job:  map 17% reduce 0%
2021-02-09 00:04:10,989 INFO mapreduce.Job:  map 18% reduce 0%
2021-02-09 00:04:12,062 INFO mapreduce.Job:  map 19% reduce 0%
2021-02-09 00:04:14,082 INFO mapreduce.Job:  map 20% reduce 0%
2021-02-09 00:04:17,127 INFO mapreduce.Job:  map 21% reduce 0%
2021-02-09 00:04:30,273 INFO mapreduce.Job:  map 22% reduce 0%
2021-02-09 00:04:33,337 INFO mapreduce.Job:  map 23% reduce 0%
2021-02-09 00:04:34,354 INFO mapreduce.Job:  map 24% reduce 0%
2021-02-09 00:04:36,412 INFO mapreduce.Job:  map 26% reduce 0%
2021-02-09 00:04:37,423 INFO mapreduce.Job:  map 26% reduce 1%
2021-02-09 00:04:38,490 INFO mapreduce.Job:  map 27% reduce 1%
2021-02-09 00:04:39,497 INFO mapreduce.Job:  map 28% reduce 1%
2021-02-09 00:04:40,501 INFO mapreduce.Job:  map 29% reduce 1%
2021-02-09 00:04:44,534 INFO mapreduce.Job:  map 30% reduce 1%
2021-02-09 00:04:55,704 INFO mapreduce.Job:  map 31% reduce 1%
2021-02-09 00:04:57,711 INFO mapreduce.Job:  map 32% reduce 1%
2021-02-09 00:04:58,715 INFO mapreduce.Job:  map 33% reduce 1%
2021-02-09 00:05:00,729 INFO mapreduce.Job:  map 34% reduce 1%
2021-02-09 00:05:01,739 INFO mapreduce.Job:  map 35% reduce 1%
2021-02-09 00:05:02,743 INFO mapreduce.Job:  map 36% reduce 1%
2021-02-09 00:05:04,765 INFO mapreduce.Job:  map 37% reduce 1%
2021-02-09 00:05:06,801 INFO mapreduce.Job:  map 37% reduce 2%
2021-02-09 00:05:14,944 INFO mapreduce.Job:  map 38% reduce 2%
2021-02-09 00:05:16,952 INFO mapreduce.Job:  map 39% reduce 2%
2021-02-09 00:05:18,961 INFO mapreduce.Job:  map 40% reduce 2%
2021-02-09 00:05:19,969 INFO mapreduce.Job:  map 41% reduce 2%
2021-02-09 00:05:20,990 INFO mapreduce.Job:  map 42% reduce 2%
2021-02-09 00:05:23,064 INFO mapreduce.Job:  map 43% reduce 2%
2021-02-09 00:05:27,099 INFO mapreduce.Job:  map 44% reduce 2%
2021-02-09 00:05:32,151 INFO mapreduce.Job:  map 45% reduce 2%
2021-02-09 00:05:35,216 INFO mapreduce.Job:  map 46% reduce 2%
2021-02-09 00:05:37,224 INFO mapreduce.Job:  map 47% reduce 2%
2021-02-09 00:05:38,273 INFO mapreduce.Job:  map 47% reduce 3%
2021-02-09 00:05:39,279 INFO mapreduce.Job:  map 48% reduce 3%
2021-02-09 00:05:43,374 INFO mapreduce.Job:  map 49% reduce 3%
2021-02-09 00:05:46,434 INFO mapreduce.Job:  map 50% reduce 3%
2021-02-09 00:05:48,465 INFO mapreduce.Job:  map 51% reduce 3%
2021-02-09 00:05:50,473 INFO mapreduce.Job:  map 52% reduce 3%
2021-02-09 00:05:51,477 INFO mapreduce.Job:  map 53% reduce 3%
2021-02-09 00:05:53,513 INFO mapreduce.Job:  map 54% reduce 3%
2021-02-09 00:05:56,569 INFO mapreduce.Job:  map 54% reduce 4%
2021-02-09 00:06:02,704 INFO mapreduce.Job:  map 55% reduce 4%
2021-02-09 00:06:03,723 INFO mapreduce.Job:  map 57% reduce 4%
2021-02-09 00:06:04,726 INFO mapreduce.Job:  map 58% reduce 4%
2021-02-09 00:06:06,764 INFO mapreduce.Job:  map 59% reduce 4%
2021-02-09 00:06:15,928 INFO mapreduce.Job:  map 60% reduce 4%
2021-02-09 00:06:16,931 INFO mapreduce.Job:  map 61% reduce 4%
2021-02-09 00:06:19,965 INFO mapreduce.Job:  map 62% reduce 4%
2021-02-09 00:06:20,968 INFO mapreduce.Job:  map 63% reduce 4%
2021-02-09 00:06:25,013 INFO mapreduce.Job:  map 63% reduce 5%
2021-02-09 00:06:26,022 INFO mapreduce.Job:  map 64% reduce 5%
2021-02-09 00:06:31,067 INFO mapreduce.Job:  map 65% reduce 5%
2021-02-09 00:06:35,078 INFO mapreduce.Job:  map 66% reduce 5%
2021-02-09 00:06:39,089 INFO mapreduce.Job:  map 67% reduce 5%
2021-02-09 00:06:46,108 INFO mapreduce.Job:  map 68% reduce 5%
2021-02-09 00:06:52,146 INFO mapreduce.Job:  map 69% reduce 5%
2021-02-09 00:07:01,183 INFO mapreduce.Job:  map 70% reduce 5%
2021-02-09 00:07:03,189 INFO mapreduce.Job:  map 71% reduce 5%
2021-02-09 00:07:14,228 INFO mapreduce.Job:  map 72% reduce 5%
2021-02-09 00:07:21,250 INFO mapreduce.Job:  map 73% reduce 5%
2021-02-09 00:07:30,278 INFO mapreduce.Job:  map 74% reduce 5%
2021-02-09 00:07:40,320 INFO mapreduce.Job:  map 75% reduce 5%
2021-02-09 00:07:49,365 INFO mapreduce.Job:  map 76% reduce 5%
2021-02-09 00:07:53,376 INFO mapreduce.Job:  map 77% reduce 5%
2021-02-09 00:07:57,392 INFO mapreduce.Job:  map 78% reduce 5%
2021-02-09 00:07:58,398 INFO mapreduce.Job:  map 78% reduce 6%
2021-02-09 00:08:02,424 INFO mapreduce.Job:  map 79% reduce 6%
2021-02-09 00:08:05,438 INFO mapreduce.Job:  map 80% reduce 6%
2021-02-09 00:08:10,564 INFO mapreduce.Job:  map 81% reduce 6%
2021-02-09 00:08:13,582 INFO mapreduce.Job:  map 82% reduce 6%
2021-02-09 00:08:16,660 INFO mapreduce.Job:  map 83% reduce 6%
2021-02-09 00:08:18,669 INFO mapreduce.Job:  map 84% reduce 6%
2021-02-09 00:08:22,682 INFO mapreduce.Job:  map 85% reduce 6%
2021-02-09 00:08:26,771 INFO mapreduce.Job:  map 86% reduce 6%
2021-02-09 00:08:28,795 INFO mapreduce.Job:  map 87% reduce 6%
2021-02-09 00:08:30,820 INFO mapreduce.Job:  map 88% reduce 6%
2021-02-09 00:08:33,838 INFO mapreduce.Job:  map 89% reduce 6%
2021-02-09 00:08:38,964 INFO mapreduce.Job:  map 90% reduce 6%
2021-02-09 00:08:42,006 INFO mapreduce.Job:  map 91% reduce 6%
2021-02-09 00:08:43,010 INFO mapreduce.Job:  map 91% reduce 7%
2021-02-09 00:08:44,027 INFO mapreduce.Job:  map 92% reduce 7%
2021-02-09 00:08:46,058 INFO mapreduce.Job:  map 93% reduce 7%
2021-02-09 00:08:51,108 INFO mapreduce.Job:  map 94% reduce 7%
2021-02-09 00:08:54,147 INFO mapreduce.Job:  map 95% reduce 7%
2021-02-09 00:08:56,173 INFO mapreduce.Job:  map 96% reduce 7%
2021-02-09 00:08:59,213 INFO mapreduce.Job:  map 97% reduce 7%
2021-02-09 00:09:02,265 INFO mapreduce.Job:  map 98% reduce 7%
2021-02-09 00:09:05,297 INFO mapreduce.Job:  map 99% reduce 7%
2021-02-09 00:09:10,354 INFO mapreduce.Job:  map 100% reduce 7%
2021-02-09 00:09:15,366 INFO mapreduce.Job:  map 100% reduce 8%
2021-02-09 00:09:21,380 INFO mapreduce.Job:  map 100% reduce 9%
2021-02-09 00:09:28,397 INFO mapreduce.Job:  map 100% reduce 10%
2021-02-09 00:09:33,409 INFO mapreduce.Job:  map 100% reduce 11%
2021-02-09 00:09:36,416 INFO mapreduce.Job:  map 100% reduce 12%
2021-02-09 00:09:40,426 INFO mapreduce.Job:  map 100% reduce 13%
2021-02-09 00:09:45,438 INFO mapreduce.Job:  map 100% reduce 14%
2021-02-09 00:09:51,452 INFO mapreduce.Job:  map 100% reduce 15%
2021-02-09 00:10:01,476 INFO mapreduce.Job:  map 100% reduce 16%
2021-02-09 00:10:13,602 INFO mapreduce.Job:  map 100% reduce 17%
2021-02-09 00:10:25,630 INFO mapreduce.Job:  map 100% reduce 18%
2021-02-09 00:10:26,633 INFO mapreduce.Job:  map 100% reduce 19%
2021-02-09 00:10:31,646 INFO mapreduce.Job:  map 100% reduce 20%
2021-02-09 00:10:37,666 INFO mapreduce.Job:  map 100% reduce 21%
2021-02-09 00:10:46,826 INFO mapreduce.Job:  map 100% reduce 4%
2021-02-09 00:10:47,838 INFO mapreduce.Job:  map 33% reduce 4%
2021-02-09 00:10:55,963 INFO mapreduce.Job:  map 34% reduce 4%
2021-02-09 00:11:08,218 INFO mapreduce.Job:  map 35% reduce 4%
2021-02-09 00:11:19,343 INFO mapreduce.Job:  map 36% reduce 4%
2021-02-09 00:11:28,585 INFO mapreduce.Job:  map 37% reduce 4%
2021-02-09 00:11:37,835 INFO mapreduce.Job:  map 38% reduce 4%
2021-02-09 00:11:43,956 INFO mapreduce.Job:  map 39% reduce 4%
2021-02-09 00:11:45,964 INFO mapreduce.Job: Task Id : attempt_1612828479085_0009_m_000102_1000, Status : FAILED
Error: org.apache.hadoop.util.DiskChecker$DiskErrorException: Could not find any valid local directory for output/attempt_1612828479085_0009_m_000102_1000/file.out
	at org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.getLocalPathForWrite(LocalDirAllocator.java:462)
	at org.apache.hadoop.fs.LocalDirAllocator.getLocalPathForWrite(LocalDirAllocator.java:165)
	at org.apache.hadoop.fs.LocalDirAllocator.getLocalPathForWrite(LocalDirAllocator.java:146)
	at org.apache.hadoop.mapred.YarnOutputFiles.getOutputFileForWrite(YarnOutputFiles.java:82)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.mergeParts(MapTask.java:1881)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.flush(MapTask.java:1527)
	at org.apache.hadoop.mapred.MapTask$NewOutputCollector.close(MapTask.java:735)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:805)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:347)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:178)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1845)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:172)

2021-02-09 00:11:45,979 INFO mapreduce.Job: Task Id : attempt_1612828479085_0009_m_000103_1000, Status : FAILED
Error: org.apache.hadoop.util.DiskChecker$DiskErrorException: Could not find any valid local directory for output/attempt_1612828479085_0009_m_000103_1000/file.out
	at org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.getLocalPathForWrite(LocalDirAllocator.java:462)
	at org.apache.hadoop.fs.LocalDirAllocator.getLocalPathForWrite(LocalDirAllocator.java:165)
	at org.apache.hadoop.fs.LocalDirAllocator.getLocalPathForWrite(LocalDirAllocator.java:146)
	at org.apache.hadoop.mapred.YarnOutputFiles.getOutputFileForWrite(YarnOutputFiles.java:82)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.mergeParts(MapTask.java:1881)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.flush(MapTask.java:1527)
	at org.apache.hadoop.mapred.MapTask$NewOutputCollector.close(MapTask.java:735)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:805)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:347)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:178)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1845)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:172)

2021-02-09 00:11:56,014 INFO mapreduce.Job:  map 40% reduce 4%
2021-02-09 00:12:09,060 INFO mapreduce.Job: Task Id : attempt_1612828479085_0009_m_000162_1000, Status : FAILED
Error: java.io.IOException: Spill failed
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.checkSpillException(MapTask.java:1586)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.collect(MapTask.java:1099)
	at org.apache.hadoop.mapred.MapTask$NewOutputCollector.write(MapTask.java:727)
	at org.apache.hadoop.mapreduce.task.TaskInputOutputContextImpl.write(TaskInputOutputContextImpl.java:89)
	at org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.write(WrappedMapper.java:112)
	at org.apache.hadoop.mapreduce.Mapper.map(Mapper.java:125)
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:146)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:799)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:347)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:178)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1845)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:172)
Caused by: org.apache.hadoop.util.DiskChecker$DiskErrorException: Could not find any valid local directory for attempt_1612828479085_0009_m_000162_1000_spill_0.out
	at org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.getLocalPathForWrite(LocalDirAllocator.java:462)
	at org.apache.hadoop.fs.LocalDirAllocator.getLocalPathForWrite(LocalDirAllocator.java:165)
	at org.apache.hadoop.fs.LocalDirAllocator.getLocalPathForWrite(LocalDirAllocator.java:146)
	at org.apache.hadoop.mapred.YarnOutputFiles.getSpillFileForWrite(YarnOutputFiles.java:157)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.sortAndSpill(MapTask.java:1617)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.access$1000(MapTask.java:888)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer$SpillThread.run(MapTask.java:1555)

2021-02-09 00:12:09,061 INFO mapreduce.Job: Task Id : attempt_1612828479085_0009_m_000112_1000, Status : FAILED
Error: org.apache.hadoop.util.DiskChecker$DiskErrorException: Could not find any valid local directory for attempt_1612828479085_0009_m_000112_1000_spill_1.out
	at org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.getLocalPathForWrite(LocalDirAllocator.java:462)
	at org.apache.hadoop.fs.LocalDirAllocator.getLocalPathForWrite(LocalDirAllocator.java:165)
	at org.apache.hadoop.fs.LocalDirAllocator.getLocalPathForWrite(LocalDirAllocator.java:146)
	at org.apache.hadoop.mapred.YarnOutputFiles.getSpillFileForWrite(YarnOutputFiles.java:157)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.sortAndSpill(MapTask.java:1617)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.flush(MapTask.java:1505)
	at org.apache.hadoop.mapred.MapTask$NewOutputCollector.close(MapTask.java:735)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:805)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:347)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:178)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1845)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:172)

2021-02-09 00:12:09,061 INFO mapreduce.Job: Task Id : attempt_1612828479085_0009_m_000158_1000, Status : FAILED
Error: java.io.IOException: Spill failed
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.checkSpillException(MapTask.java:1586)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.collect(MapTask.java:1099)
	at org.apache.hadoop.mapred.MapTask$NewOutputCollector.write(MapTask.java:727)
	at org.apache.hadoop.mapreduce.task.TaskInputOutputContextImpl.write(TaskInputOutputContextImpl.java:89)
	at org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.write(WrappedMapper.java:112)
	at org.apache.hadoop.mapreduce.Mapper.map(Mapper.java:125)
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:146)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:799)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:347)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:178)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1845)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:172)
Caused by: org.apache.hadoop.util.DiskChecker$DiskErrorException: Could not find any valid local directory for attempt_1612828479085_0009_m_000158_1000_spill_0.out
	at org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.getLocalPathForWrite(LocalDirAllocator.java:462)
	at org.apache.hadoop.fs.LocalDirAllocator.getLocalPathForWrite(LocalDirAllocator.java:165)
	at org.apache.hadoop.fs.LocalDirAllocator.getLocalPathForWrite(LocalDirAllocator.java:146)
	at org.apache.hadoop.mapred.YarnOutputFiles.getSpillFileForWrite(YarnOutputFiles.java:157)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.sortAndSpill(MapTask.java:1617)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.access$1000(MapTask.java:888)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer$SpillThread.run(MapTask.java:1555)

2021-02-09 00:12:09,062 INFO mapreduce.Job: Task Id : attempt_1612828479085_0009_m_000153_1000, Status : FAILED
Error: java.io.IOException: Spill failed
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.checkSpillException(MapTask.java:1586)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.collect(MapTask.java:1099)
	at org.apache.hadoop.mapred.MapTask$NewOutputCollector.write(MapTask.java:727)
	at org.apache.hadoop.mapreduce.task.TaskInputOutputContextImpl.write(TaskInputOutputContextImpl.java:89)
	at org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.write(WrappedMapper.java:112)
	at org.apache.hadoop.mapreduce.Mapper.map(Mapper.java:125)
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:146)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:799)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:347)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:178)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1845)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:172)
Caused by: org.apache.hadoop.util.DiskChecker$DiskErrorException: Could not find any valid local directory for attempt_1612828479085_0009_m_000153_1000_spill_0.out
	at org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.getLocalPathForWrite(LocalDirAllocator.java:462)
	at org.apache.hadoop.fs.LocalDirAllocator.getLocalPathForWrite(LocalDirAllocator.java:165)
	at org.apache.hadoop.fs.LocalDirAllocator.getLocalPathForWrite(LocalDirAllocator.java:146)
	at org.apache.hadoop.mapred.YarnOutputFiles.getSpillFileForWrite(YarnOutputFiles.java:157)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.sortAndSpill(MapTask.java:1617)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.access$1000(MapTask.java:888)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer$SpillThread.run(MapTask.java:1555)

2021-02-09 00:12:09,063 INFO mapreduce.Job: Task Id : attempt_1612828479085_0009_m_000100_1000, Status : FAILED
FSError: java.io.IOException: No space left on device
2021-02-09 00:12:09,063 INFO mapreduce.Job: Task Id : attempt_1612828479085_0009_m_000101_1000, Status : FAILED
FSError: java.io.IOException: No space left on device
2021-02-09 00:12:09,064 INFO mapreduce.Job: Task Id : attempt_1612828479085_0009_m_000108_1000, Status : FAILED
FSError: java.io.IOException: No space left on device
2021-02-09 00:12:09,065 INFO mapreduce.Job: Task Id : attempt_1612828479085_0009_m_000111_1000, Status : FAILED
FSError: java.io.IOException: No space left on device
2021-02-09 00:12:09,065 INFO mapreduce.Job: Task Id : attempt_1612828479085_0009_m_000114_1000, Status : FAILED
Error: java.io.IOException: No space left on device
	at java.io.FileOutputStream.writeBytes(Native Method)
	at java.io.FileOutputStream.write(FileOutputStream.java:326)
	at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.write(RawLocalFileSystem.java:275)
	at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)
	at java.io.BufferedOutputStream.flush(BufferedOutputStream.java:140)
	at java.io.FilterOutputStream.close(FilterOutputStream.java:158)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:101)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.sortAndSpill(MapTask.java:1699)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.access$1000(MapTask.java:888)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer$SpillThread.run(MapTask.java:1555)

Error: java.io.IOException: No space left on device
	at java.io.FileOutputStream.writeBytes(Native Method)
	at java.io.FileOutputStream.write(FileOutputStream.java:326)
	at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.write(RawLocalFileSystem.java:275)
	at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)
	at java.io.BufferedOutputStream.flush(BufferedOutputStream.java:140)
	at java.io.FilterOutputStream.close(FilterOutputStream.java:158)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:101)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.sortAndSpill(MapTask.java:1699)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.access$1000(MapTask.java:888)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer$SpillThread.run(MapTask.java:1555)

Error: java.io.IOException: Spill failed
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.checkSpillException(MapTask.java:1586)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.access$300(MapTask.java:888)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer$Buffer.write(MapTask.java:1388)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer$Buffer.write(MapTask.java:1365)
	at java.io.DataOutputStream.writeByte(DataOutputStream.java:153)
	at org.apache.hadoop.io.WritableUtils.writeVLong(WritableUtils.java:275)
	at org.apache.hadoop.io.WritableUtils.writeVInt(WritableUtils.java:254)
	at org.apache.hadoop.io.Text.write(Text.java:330)
	at org.apache.hadoop.io.serializer.WritableSerialization$WritableSerializer.serialize(WritableSerialization.java:98)
	at org.apache.hadoop.io.serializer.WritableSerialization$WritableSerializer.serialize(WritableSerialization.java:82)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.collect(MapTask.java:1163)
	at org.apache.hadoop.mapred.MapTask$NewOutputCollector.write(MapTask.java:727)
	at org.apache.hadoop.mapreduce.task.TaskInputOutputContextImpl.write(TaskInputOutputContextImpl.java:89)
	at org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.write(WrappedMapper.java:112)
	at org.apache.hadoop.mapreduce.Mapper.map(Mapper.java:125)
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:146)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:799)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:347)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:178)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1845)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:172)
Caused by: org.apache.hadoop.fs.FSError: java.io.IOException: No space left on device
	at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.write(RawLocalFileSystem.java:277)
	at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)
	at java.io.BufferedOutputStream.flush(BufferedOutputStream.java:140)
	at java.io.FilterOutputStream.close(FilterOutputStream.java:158)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:101)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.sortAndSpill(MapTask.java:1699)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.access$1000(MapTask.java:888)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer$SpillThread.run(MapTask.java:1555)
Caused by: java.io.IOException: No space left on device
	at java.io.FileOutputStream.writeBytes(Native Method)
	at java.io.FileOutputStream.write(FileOutputStream.java:326)
	at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.write(RawLocalFileSystem.java:275)
	... 8 more

2021-02-09 00:12:09,066 INFO mapreduce.Job: Task Id : attempt_1612828479085_0009_m_000113_1000, Status : FAILED
Error: java.io.IOException: No space left on device
	at java.io.FileOutputStream.writeBytes(Native Method)
	at java.io.FileOutputStream.write(FileOutputStream.java:326)
	at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.write(RawLocalFileSystem.java:275)
	at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)
	at java.io.BufferedOutputStream.flush(BufferedOutputStream.java:140)
	at java.io.FilterOutputStream.close(FilterOutputStream.java:158)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:101)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.sortAndSpill(MapTask.java:1699)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.access$1000(MapTask.java:888)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer$SpillThread.run(MapTask.java:1555)

Error: java.io.IOException: No space left on device
	at java.io.FileOutputStream.writeBytes(Native Method)
	at java.io.FileOutputStream.write(FileOutputStream.java:326)
	at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.write(RawLocalFileSystem.java:275)
	at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)
	at java.io.BufferedOutputStream.flush(BufferedOutputStream.java:140)
	at java.io.FilterOutputStream.close(FilterOutputStream.java:158)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:101)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.sortAndSpill(MapTask.java:1699)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.access$1000(MapTask.java:888)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer$SpillThread.run(MapTask.java:1555)

Error: java.io.IOException: Spill failed
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.checkSpillException(MapTask.java:1586)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.access$300(MapTask.java:888)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer$Buffer.write(MapTask.java:1388)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer$Buffer.write(MapTask.java:1365)
	at java.io.DataOutputStream.writeByte(DataOutputStream.java:153)
	at org.apache.hadoop.io.WritableUtils.writeVLong(WritableUtils.java:275)
	at org.apache.hadoop.io.WritableUtils.writeVInt(WritableUtils.java:254)
	at org.apache.hadoop.io.Text.write(Text.java:330)
	at org.apache.hadoop.io.serializer.WritableSerialization$WritableSerializer.serialize(WritableSerialization.java:98)
	at org.apache.hadoop.io.serializer.WritableSerialization$WritableSerializer.serialize(WritableSerialization.java:82)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.collect(MapTask.java:1163)
	at org.apache.hadoop.mapred.MapTask$NewOutputCollector.write(MapTask.java:727)
	at org.apache.hadoop.mapreduce.task.TaskInputOutputContextImpl.write(TaskInputOutputContextImpl.java:89)
	at org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.write(WrappedMapper.java:112)
	at org.apache.hadoop.mapreduce.Mapper.map(Mapper.java:125)
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:146)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:799)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:347)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:178)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1845)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:172)
Caused by: org.apache.hadoop.fs.FSError: java.io.IOException: No space left on device
	at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.write(RawLocalFileSystem.java:277)
	at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)
	at java.io.BufferedOutputStream.flush(BufferedOutputStream.java:140)
	at java.io.FilterOutputStream.close(FilterOutputStream.java:158)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:101)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.sortAndSpill(MapTask.java:1699)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.access$1000(MapTask.java:888)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer$SpillThread.run(MapTask.java:1555)
Caused by: java.io.IOException: No space left on device
	at java.io.FileOutputStream.writeBytes(Native Method)
	at java.io.FileOutputStream.write(FileOutputStream.java:326)
	at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.write(RawLocalFileSystem.java:275)
	... 8 more

2021-02-09 00:12:10,091 INFO mapreduce.Job:  map 38% reduce 4%
2021-02-09 00:12:10,112 INFO mapreduce.Job: Task Id : attempt_1612828479085_0009_m_000168_1000, Status : FAILED
Error: java.io.FileNotFoundException: /usr/lib/hadoop/tmp/nm-local-dir/usercache/ec2-user/appcache/application_1612828479085_0009/container_1612828479085_0009_02_000060/.job.xml.crc (No space left on device)
	at java.io.FileOutputStream.open0(Native Method)
	at java.io.FileOutputStream.open(FileOutputStream.java:270)
	at java.io.FileOutputStream.<init>(FileOutputStream.java:213)
	at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:251)
	at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:234)
	at org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:333)
	at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:322)
	at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:353)
	at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:420)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:479)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:458)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1164)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1144)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1033)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1021)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:697)
	at org.apache.hadoop.mapred.YarnChild.writeLocalJobFile(YarnChild.java:373)
	at org.apache.hadoop.mapred.YarnChild.configureTask(YarnChild.java:355)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:150)

2021-02-09 00:12:10,113 INFO mapreduce.Job: Task Id : attempt_1612828479085_0009_m_000107_1000, Status : FAILED
FSError: java.io.IOException: No space left on device
2021-02-09 00:12:10,114 INFO mapreduce.Job: Task Id : attempt_1612828479085_0009_m_000116_1000, Status : FAILED
Error: java.io.IOException: No space left on device
	at java.io.FileOutputStream.writeBytes(Native Method)
	at java.io.FileOutputStream.write(FileOutputStream.java:326)
	at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.write(RawLocalFileSystem.java:275)
	at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)
	at java.io.BufferedOutputStream.flush(BufferedOutputStream.java:140)
	at java.io.FilterOutputStream.close(FilterOutputStream.java:158)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:101)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.sortAndSpill(MapTask.java:1699)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.access$1000(MapTask.java:888)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer$SpillThread.run(MapTask.java:1555)

Error: java.io.IOException: No space left on device
	at java.io.FileOutputStream.writeBytes(Native Method)
	at java.io.FileOutputStream.write(FileOutputStream.java:326)
	at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.write(RawLocalFileSystem.java:275)
	at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)
	at java.io.BufferedOutputStream.flush(BufferedOutputStream.java:140)
	at java.io.FilterOutputStream.close(FilterOutputStream.java:158)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:101)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.sortAndSpill(MapTask.java:1699)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.access$1000(MapTask.java:888)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer$SpillThread.run(MapTask.java:1555)

Error: java.io.IOException: Spill failed
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.checkSpillException(MapTask.java:1586)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.access$300(MapTask.java:888)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer$Buffer.write(MapTask.java:1388)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer$Buffer.write(MapTask.java:1365)
	at java.io.DataOutputStream.writeByte(DataOutputStream.java:153)
	at org.apache.hadoop.io.WritableUtils.writeVLong(WritableUtils.java:275)
	at org.apache.hadoop.io.WritableUtils.writeVInt(WritableUtils.java:254)
	at org.apache.hadoop.io.Text.write(Text.java:330)
	at org.apache.hadoop.io.serializer.WritableSerialization$WritableSerializer.serialize(WritableSerialization.java:98)
	at org.apache.hadoop.io.serializer.WritableSerialization$WritableSerializer.serialize(WritableSerialization.java:82)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.collect(MapTask.java:1163)
	at org.apache.hadoop.mapred.MapTask$NewOutputCollector.write(MapTask.java:727)
	at org.apache.hadoop.mapreduce.task.TaskInputOutputContextImpl.write(TaskInputOutputContextImpl.java:89)
	at org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.write(WrappedMapper.java:112)
	at org.apache.hadoop.mapreduce.Mapper.map(Mapper.java:125)
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:146)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:799)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:347)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:178)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1845)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:172)
Caused by: org.apache.hadoop.fs.FSError: java.io.IOException: No space left on device
	at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.write(RawLocalFileSystem.java:277)
	at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)
	at java.io.BufferedOutputStream.flush(BufferedOutputStream.java:140)
	at java.io.FilterOutputStream.close(FilterOutputStream.java:158)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:101)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.sortAndSpill(MapTask.java:1699)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.access$1000(MapTask.java:888)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer$SpillThread.run(MapTask.java:1555)
Caused by: java.io.IOException: No space left on device
	at java.io.FileOutputStream.writeBytes(Native Method)
	at java.io.FileOutputStream.write(FileOutputStream.java:326)
	at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.write(RawLocalFileSystem.java:275)
	... 8 more

2021-02-09 00:12:10,115 INFO mapreduce.Job: Task Id : attempt_1612828479085_0009_m_000105_1000, Status : FAILED
FSError: java.io.IOException: No space left on device
2021-02-09 00:12:10,117 INFO mapreduce.Job: Task Id : attempt_1612828479085_0009_m_000110_1000, Status : FAILED
FSError: java.io.IOException: No space left on device
2021-02-09 00:12:10,117 INFO mapreduce.Job: Task Id : attempt_1612828479085_0009_m_000109_1000, Status : FAILED
FSError: java.io.IOException: No space left on device
2021-02-09 00:12:10,118 INFO mapreduce.Job: Task Id : attempt_1612828479085_0009_m_000177_1000, Status : FAILED
[2021-02-09 00:12:08.823]Exception from container-launch.
Container id: container_1612828479085_0009_02_000065
Exit code: 1
Exception message: /usr/lib/hadoop/tmp/nm-local-dir/usercache/ec2-user/appcache/application_1612828479085_0009/container_1612828479085_0009_02_000065/launch_container.sh: line 5: /usr/lib/hadoop/logs/userlogs/application_1612828479085_0009/container_1612828479085_0009_02_000065/prelaunch.out: No space left on device
/usr/lib/hadoop/tmp/nm-local-dir/usercache/ec2-user/appcache/application_1612828479085_0009/container_1612828479085_0009_02_000065/default_container_executor.sh: line 4: /usr/lib/hadoop/tmp/nm-local-dir/nmPrivate/application_1612828479085_0009/container_1612828479085_0009_02_000065/container_1612828479085_0009_02_000065.pid.exitcode.tmp: No space left on device
/bin/mv: cannot stat '/usr/lib/hadoop/tmp/nm-local-dir/nmPrivate/application_1612828479085_0009/container_1612828479085_0009_02_000065/container_1612828479085_0009_02_000065.pid.exitcode.tmp': No such file or directory


[2021-02-09 00:12:08.886]Container exited with a non-zero exit code 1. 
[2021-02-09 00:12:08.887]Container exited with a non-zero exit code 1. 

2021-02-09 00:12:10,119 INFO mapreduce.Job: Task Id : attempt_1612828479085_0009_m_000175_1000, Status : FAILED
[2021-02-09 00:12:08.832]Exception from container-launch.
Container id: container_1612828479085_0009_02_000064
Exit code: 1
Exception message: /usr/lib/hadoop/tmp/nm-local-dir/usercache/ec2-user/appcache/application_1612828479085_0009/container_1612828479085_0009_02_000064/default_container_executor.sh: line 4: /usr/lib/hadoop/tmp/nm-local-dir/nmPrivate/application_1612828479085_0009/container_1612828479085_0009_02_000064/container_1612828479085_0009_02_000064.pid.exitcode.tmp: No space left on device
/bin/mv: cannot stat '/usr/lib/hadoop/tmp/nm-local-dir/nmPrivate/application_1612828479085_0009/container_1612828479085_0009_02_000064/container_1612828479085_0009_02_000064.pid.exitcode.tmp': No such file or directory


[2021-02-09 00:12:08.882]Container exited with a non-zero exit code 1. Error file: prelaunch.err.
Last 4096 bytes of prelaunch.err :
/usr/lib/hadoop/tmp/nm-local-dir/usercache/ec2-user/appcache/application_1612828479085_0009/container_1612828479085_0009_02_000064/launch_container.sh: line 49: /usr/lib/hadoop/logs/userlogs/application_1612828479085_0009/container_1612828479085_0009_02_000064/directory.info: No space left on device

[2021-02-09 00:12:08.882]Container exited with a non-zero exit code 1. Error file: prelaunch.err.
Last 4096 bytes of prelaunch.err :
/usr/lib/hadoop/tmp/nm-local-dir/usercache/ec2-user/appcache/application_1612828479085_0009/container_1612828479085_0009_02_000064/launch_container.sh: line 49: /usr/lib/hadoop/logs/userlogs/application_1612828479085_0009/container_1612828479085_0009_02_000064/directory.info: No space left on device


2021-02-09 00:12:10,119 INFO mapreduce.Job: Task Id : attempt_1612828479085_0009_m_000164_1000, Status : FAILED
Error: java.io.IOException: Spill failed
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.checkSpillException(MapTask.java:1586)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.collect(MapTask.java:1099)
	at org.apache.hadoop.mapred.MapTask$NewOutputCollector.write(MapTask.java:727)
	at org.apache.hadoop.mapreduce.task.TaskInputOutputContextImpl.write(TaskInputOutputContextImpl.java:89)
	at org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.write(WrappedMapper.java:112)
	at org.apache.hadoop.mapreduce.Mapper.map(Mapper.java:125)
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:146)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:799)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:347)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:178)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1845)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:172)
Caused by: org.apache.hadoop.util.DiskChecker$DiskErrorException: Could not find any valid local directory for attempt_1612828479085_0009_m_000164_1000_spill_0.out
	at org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.getLocalPathForWrite(LocalDirAllocator.java:462)
	at org.apache.hadoop.fs.LocalDirAllocator.getLocalPathForWrite(LocalDirAllocator.java:165)
	at org.apache.hadoop.fs.LocalDirAllocator.getLocalPathForWrite(LocalDirAllocator.java:146)
	at org.apache.hadoop.mapred.YarnOutputFiles.getSpillFileForWrite(YarnOutputFiles.java:157)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.sortAndSpill(MapTask.java:1617)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.access$1000(MapTask.java:888)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer$SpillThread.run(MapTask.java:1555)

2021-02-09 00:12:11,123 INFO mapreduce.Job: Task Id : attempt_1612828479085_0009_m_000165_1000, Status : FAILED
Error: java.io.IOException: Spill failed
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.checkSpillException(MapTask.java:1586)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.collect(MapTask.java:1099)
	at org.apache.hadoop.mapred.MapTask$NewOutputCollector.write(MapTask.java:727)
	at org.apache.hadoop.mapreduce.task.TaskInputOutputContextImpl.write(TaskInputOutputContextImpl.java:89)
	at org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.write(WrappedMapper.java:112)
	at org.apache.hadoop.mapreduce.Mapper.map(Mapper.java:125)
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:146)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:799)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:347)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:178)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1845)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:172)
Caused by: org.apache.hadoop.util.DiskChecker$DiskErrorException: Could not find any valid local directory for attempt_1612828479085_0009_m_000165_1000_spill_0.out
	at org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.getLocalPathForWrite(LocalDirAllocator.java:462)
	at org.apache.hadoop.fs.LocalDirAllocator.getLocalPathForWrite(LocalDirAllocator.java:165)
	at org.apache.hadoop.fs.LocalDirAllocator.getLocalPathForWrite(LocalDirAllocator.java:146)
	at org.apache.hadoop.mapred.YarnOutputFiles.getSpillFileForWrite(YarnOutputFiles.java:157)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.sortAndSpill(MapTask.java:1617)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.access$1000(MapTask.java:888)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer$SpillThread.run(MapTask.java:1555)

2021-02-09 00:12:11,124 INFO mapreduce.Job: Task Id : attempt_1612828479085_0009_m_000167_1000, Status : FAILED
Error: java.io.IOException: Spill failed
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.checkSpillException(MapTask.java:1586)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.collect(MapTask.java:1099)
	at org.apache.hadoop.mapred.MapTask$NewOutputCollector.write(MapTask.java:727)
	at org.apache.hadoop.mapreduce.task.TaskInputOutputContextImpl.write(TaskInputOutputContextImpl.java:89)
	at org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.write(WrappedMapper.java:112)
	at org.apache.hadoop.mapreduce.Mapper.map(Mapper.java:125)
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:146)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:799)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:347)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:178)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1845)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:172)
Caused by: org.apache.hadoop.util.DiskChecker$DiskErrorException: Could not find any valid local directory for attempt_1612828479085_0009_m_000167_1000_spill_0.out
	at org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.getLocalPathForWrite(LocalDirAllocator.java:462)
	at org.apache.hadoop.fs.LocalDirAllocator.getLocalPathForWrite(LocalDirAllocator.java:165)
	at org.apache.hadoop.fs.LocalDirAllocator.getLocalPathForWrite(LocalDirAllocator.java:146)
	at org.apache.hadoop.mapred.YarnOutputFiles.getSpillFileForWrite(YarnOutputFiles.java:157)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.sortAndSpill(MapTask.java:1617)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.access$1000(MapTask.java:888)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer$SpillThread.run(MapTask.java:1555)

2021-02-09 00:12:13,144 INFO mapreduce.Job: Task Id : attempt_1612828479085_0009_m_000171_1000, Status : FAILED
Error: java.io.IOException: Spill failed
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.checkSpillException(MapTask.java:1586)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.collect(MapTask.java:1099)
	at org.apache.hadoop.mapred.MapTask$NewOutputCollector.write(MapTask.java:727)
	at org.apache.hadoop.mapreduce.task.TaskInputOutputContextImpl.write(TaskInputOutputContextImpl.java:89)
	at org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.write(WrappedMapper.java:112)
	at org.apache.hadoop.mapreduce.Mapper.map(Mapper.java:125)
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:146)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:799)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:347)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:178)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1845)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:172)
Caused by: org.apache.hadoop.util.DiskChecker$DiskErrorException: Could not find any valid local directory for attempt_1612828479085_0009_m_000171_1000_spill_0.out
	at org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.getLocalPathForWrite(LocalDirAllocator.java:462)
	at org.apache.hadoop.fs.LocalDirAllocator.getLocalPathForWrite(LocalDirAllocator.java:165)
	at org.apache.hadoop.fs.LocalDirAllocator.getLocalPathForWrite(LocalDirAllocator.java:146)
	at org.apache.hadoop.mapred.YarnOutputFiles.getSpillFileForWrite(YarnOutputFiles.java:157)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.sortAndSpill(MapTask.java:1617)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.access$1000(MapTask.java:888)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer$SpillThread.run(MapTask.java:1555)

2021-02-09 00:12:13,144 INFO mapreduce.Job: Task Id : attempt_1612828479085_0009_m_000173_1000, Status : FAILED
Error: java.io.IOException: Spill failed
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.checkSpillException(MapTask.java:1586)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.collect(MapTask.java:1099)
	at org.apache.hadoop.mapred.MapTask$NewOutputCollector.write(MapTask.java:727)
	at org.apache.hadoop.mapreduce.task.TaskInputOutputContextImpl.write(TaskInputOutputContextImpl.java:89)
	at org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.write(WrappedMapper.java:112)
	at org.apache.hadoop.mapreduce.Mapper.map(Mapper.java:125)
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:146)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:799)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:347)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:178)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1845)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:172)
Caused by: org.apache.hadoop.util.DiskChecker$DiskErrorException: Could not find any valid local directory for attempt_1612828479085_0009_m_000173_1000_spill_0.out
	at org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.getLocalPathForWrite(LocalDirAllocator.java:462)
	at org.apache.hadoop.fs.LocalDirAllocator.getLocalPathForWrite(LocalDirAllocator.java:165)
	at org.apache.hadoop.fs.LocalDirAllocator.getLocalPathForWrite(LocalDirAllocator.java:146)
	at org.apache.hadoop.mapred.YarnOutputFiles.getSpillFileForWrite(YarnOutputFiles.java:157)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.sortAndSpill(MapTask.java:1617)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.access$1000(MapTask.java:888)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer$SpillThread.run(MapTask.java:1555)

2021-02-09 00:12:14,153 INFO mapreduce.Job: Task Id : attempt_1612828479085_0009_m_000174_1000, Status : FAILED
Error: java.io.IOException: Spill failed
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.checkSpillException(MapTask.java:1586)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.collect(MapTask.java:1099)
	at org.apache.hadoop.mapred.MapTask$NewOutputCollector.write(MapTask.java:727)
	at org.apache.hadoop.mapreduce.task.TaskInputOutputContextImpl.write(TaskInputOutputContextImpl.java:89)
	at org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.write(WrappedMapper.java:112)
	at org.apache.hadoop.mapreduce.Mapper.map(Mapper.java:125)
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:146)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:799)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:347)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:178)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1845)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:172)
Caused by: org.apache.hadoop.util.DiskChecker$DiskErrorException: Could not find any valid local directory for attempt_1612828479085_0009_m_000174_1000_spill_0.out
	at org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.getLocalPathForWrite(LocalDirAllocator.java:462)
	at org.apache.hadoop.fs.LocalDirAllocator.getLocalPathForWrite(LocalDirAllocator.java:165)
	at org.apache.hadoop.fs.LocalDirAllocator.getLocalPathForWrite(LocalDirAllocator.java:146)
	at org.apache.hadoop.mapred.YarnOutputFiles.getSpillFileForWrite(YarnOutputFiles.java:157)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.sortAndSpill(MapTask.java:1617)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.access$1000(MapTask.java:888)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer$SpillThread.run(MapTask.java:1555)

2021-02-09 00:12:15,161 INFO mapreduce.Job: Task Id : attempt_1612828479085_0009_m_000213_1000, Status : FAILED
Error: java.io.IOException: Spill failed
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.checkSpillException(MapTask.java:1586)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.collect(MapTask.java:1099)
	at org.apache.hadoop.mapred.MapTask$NewOutputCollector.write(MapTask.java:727)
	at org.apache.hadoop.mapreduce.task.TaskInputOutputContextImpl.write(TaskInputOutputContextImpl.java:89)
	at org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.write(WrappedMapper.java:112)
	at org.apache.hadoop.mapreduce.Mapper.map(Mapper.java:125)
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:146)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:799)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:347)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:178)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1845)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:172)
Caused by: org.apache.hadoop.util.DiskChecker$DiskErrorException: Could not find any valid local directory for attempt_1612828479085_0009_m_000213_1000_spill_0.out
	at org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.getLocalPathForWrite(LocalDirAllocator.java:462)
	at org.apache.hadoop.fs.LocalDirAllocator.getLocalPathForWrite(LocalDirAllocator.java:165)
	at org.apache.hadoop.fs.LocalDirAllocator.getLocalPathForWrite(LocalDirAllocator.java:146)
	at org.apache.hadoop.mapred.YarnOutputFiles.getSpillFileForWrite(YarnOutputFiles.java:157)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.sortAndSpill(MapTask.java:1617)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.access$1000(MapTask.java:888)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer$SpillThread.run(MapTask.java:1555)

2021-02-09 00:12:16,193 INFO mapreduce.Job: Task Id : attempt_1612828479085_0009_m_000181_1000, Status : FAILED
Error: java.io.IOException: Spill failed
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.checkSpillException(MapTask.java:1586)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.collect(MapTask.java:1099)
	at org.apache.hadoop.mapred.MapTask$NewOutputCollector.write(MapTask.java:727)
	at org.apache.hadoop.mapreduce.task.TaskInputOutputContextImpl.write(TaskInputOutputContextImpl.java:89)
	at org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.write(WrappedMapper.java:112)
	at org.apache.hadoop.mapreduce.Mapper.map(Mapper.java:125)
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:146)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:799)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:347)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:178)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1845)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:172)
Caused by: org.apache.hadoop.util.DiskChecker$DiskErrorException: Could not find any valid local directory for attempt_1612828479085_0009_m_000181_1000_spill_0.out
	at org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.getLocalPathForWrite(LocalDirAllocator.java:462)
	at org.apache.hadoop.fs.LocalDirAllocator.getLocalPathForWrite(LocalDirAllocator.java:165)
	at org.apache.hadoop.fs.LocalDirAllocator.getLocalPathForWrite(LocalDirAllocator.java:146)
	at org.apache.hadoop.mapred.YarnOutputFiles.getSpillFileForWrite(YarnOutputFiles.java:157)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.sortAndSpill(MapTask.java:1617)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.access$1000(MapTask.java:888)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer$SpillThread.run(MapTask.java:1555)

2021-02-09 00:12:17,246 INFO mapreduce.Job: Task Id : attempt_1612828479085_0009_m_000183_1000, Status : FAILED
Error: java.io.IOException: Spill failed
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.checkSpillException(MapTask.java:1586)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.collect(MapTask.java:1099)
	at org.apache.hadoop.mapred.MapTask$NewOutputCollector.write(MapTask.java:727)
	at org.apache.hadoop.mapreduce.task.TaskInputOutputContextImpl.write(TaskInputOutputContextImpl.java:89)
	at org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.write(WrappedMapper.java:112)
	at org.apache.hadoop.mapreduce.Mapper.map(Mapper.java:125)
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:146)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:799)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:347)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:178)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1845)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:172)
Caused by: org.apache.hadoop.util.DiskChecker$DiskErrorException: Could not find any valid local directory for attempt_1612828479085_0009_m_000183_1000_spill_0.out
	at org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.getLocalPathForWrite(LocalDirAllocator.java:462)
	at org.apache.hadoop.fs.LocalDirAllocator.getLocalPathForWrite(LocalDirAllocator.java:165)
	at org.apache.hadoop.fs.LocalDirAllocator.getLocalPathForWrite(LocalDirAllocator.java:146)
	at org.apache.hadoop.mapred.YarnOutputFiles.getSpillFileForWrite(YarnOutputFiles.java:157)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.sortAndSpill(MapTask.java:1617)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.access$1000(MapTask.java:888)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer$SpillThread.run(MapTask.java:1555)

2021-02-09 00:12:17,247 INFO mapreduce.Job: Task Id : attempt_1612828479085_0009_m_000178_1000, Status : FAILED
Error: java.io.IOException: Spill failed
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.checkSpillException(MapTask.java:1586)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.collect(MapTask.java:1099)
	at org.apache.hadoop.mapred.MapTask$NewOutputCollector.write(MapTask.java:727)
	at org.apache.hadoop.mapreduce.task.TaskInputOutputContextImpl.write(TaskInputOutputContextImpl.java:89)
	at org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.write(WrappedMapper.java:112)
	at org.apache.hadoop.mapreduce.Mapper.map(Mapper.java:125)
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:146)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:799)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:347)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:178)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1845)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:172)
Caused by: org.apache.hadoop.util.DiskChecker$DiskErrorException: Could not find any valid local directory for attempt_1612828479085_0009_m_000178_1000_spill_0.out
	at org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.getLocalPathForWrite(LocalDirAllocator.java:462)
	at org.apache.hadoop.fs.LocalDirAllocator.getLocalPathForWrite(LocalDirAllocator.java:165)
	at org.apache.hadoop.fs.LocalDirAllocator.getLocalPathForWrite(LocalDirAllocator.java:146)
	at org.apache.hadoop.mapred.YarnOutputFiles.getSpillFileForWrite(YarnOutputFiles.java:157)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.sortAndSpill(MapTask.java:1617)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.access$1000(MapTask.java:888)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer$SpillThread.run(MapTask.java:1555)

2021-02-09 00:12:17,248 INFO mapreduce.Job: Task Id : attempt_1612828479085_0009_m_000203_1000, Status : FAILED
Error: java.io.IOException: Spill failed
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.checkSpillException(MapTask.java:1586)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.collect(MapTask.java:1099)
	at org.apache.hadoop.mapred.MapTask$NewOutputCollector.write(MapTask.java:727)
	at org.apache.hadoop.mapreduce.task.TaskInputOutputContextImpl.write(TaskInputOutputContextImpl.java:89)
	at org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.write(WrappedMapper.java:112)
	at org.apache.hadoop.mapreduce.Mapper.map(Mapper.java:125)
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:146)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:799)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:347)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:178)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1845)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:172)
Caused by: org.apache.hadoop.util.DiskChecker$DiskErrorException: Could not find any valid local directory for attempt_1612828479085_0009_m_000203_1000_spill_0.out
	at org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.getLocalPathForWrite(LocalDirAllocator.java:462)
	at org.apache.hadoop.fs.LocalDirAllocator.getLocalPathForWrite(LocalDirAllocator.java:165)
	at org.apache.hadoop.fs.LocalDirAllocator.getLocalPathForWrite(LocalDirAllocator.java:146)
	at org.apache.hadoop.mapred.YarnOutputFiles.getSpillFileForWrite(YarnOutputFiles.java:157)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.sortAndSpill(MapTask.java:1617)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.access$1000(MapTask.java:888)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer$SpillThread.run(MapTask.java:1555)

2021-02-09 00:12:17,249 INFO mapreduce.Job: Task Id : attempt_1612828479085_0009_m_000208_1000, Status : FAILED
Error: java.io.IOException: Spill failed
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.checkSpillException(MapTask.java:1586)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.collect(MapTask.java:1099)
	at org.apache.hadoop.mapred.MapTask$NewOutputCollector.write(MapTask.java:727)
	at org.apache.hadoop.mapreduce.task.TaskInputOutputContextImpl.write(TaskInputOutputContextImpl.java:89)
	at org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.write(WrappedMapper.java:112)
	at org.apache.hadoop.mapreduce.Mapper.map(Mapper.java:125)
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:146)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:799)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:347)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:178)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1845)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:172)
Caused by: org.apache.hadoop.util.DiskChecker$DiskErrorException: Could not find any valid local directory for attempt_1612828479085_0009_m_000208_1000_spill_0.out
	at org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.getLocalPathForWrite(LocalDirAllocator.java:462)
	at org.apache.hadoop.fs.LocalDirAllocator.getLocalPathForWrite(LocalDirAllocator.java:165)
	at org.apache.hadoop.fs.LocalDirAllocator.getLocalPathForWrite(LocalDirAllocator.java:146)
	at org.apache.hadoop.mapred.YarnOutputFiles.getSpillFileForWrite(YarnOutputFiles.java:157)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.sortAndSpill(MapTask.java:1617)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.access$1000(MapTask.java:888)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer$SpillThread.run(MapTask.java:1555)

2021-02-09 00:12:17,249 INFO mapreduce.Job: Task Id : attempt_1612828479085_0009_m_000199_1000, Status : FAILED
Error: java.io.IOException: Spill failed
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.checkSpillException(MapTask.java:1586)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.collect(MapTask.java:1099)
	at org.apache.hadoop.mapred.MapTask$NewOutputCollector.write(MapTask.java:727)
	at org.apache.hadoop.mapreduce.task.TaskInputOutputContextImpl.write(TaskInputOutputContextImpl.java:89)
	at org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.write(WrappedMapper.java:112)
	at org.apache.hadoop.mapreduce.Mapper.map(Mapper.java:125)
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:146)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:799)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:347)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:178)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1845)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:172)
Caused by: org.apache.hadoop.util.DiskChecker$DiskErrorException: Could not find any valid local directory for attempt_1612828479085_0009_m_000199_1000_spill_0.out
	at org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.getLocalPathForWrite(LocalDirAllocator.java:462)
	at org.apache.hadoop.fs.LocalDirAllocator.getLocalPathForWrite(LocalDirAllocator.java:165)
	at org.apache.hadoop.fs.LocalDirAllocator.getLocalPathForWrite(LocalDirAllocator.java:146)
	at org.apache.hadoop.mapred.YarnOutputFiles.getSpillFileForWrite(YarnOutputFiles.java:157)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.sortAndSpill(MapTask.java:1617)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.access$1000(MapTask.java:888)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer$SpillThread.run(MapTask.java:1555)

2021-02-09 00:12:19,272 INFO mapreduce.Job: Task Id : attempt_1612828479085_0009_m_000221_1000, Status : FAILED
Error: java.io.IOException: Spill failed
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.checkSpillException(MapTask.java:1586)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.collect(MapTask.java:1099)
	at org.apache.hadoop.mapred.MapTask$NewOutputCollector.write(MapTask.java:727)
	at org.apache.hadoop.mapreduce.task.TaskInputOutputContextImpl.write(TaskInputOutputContextImpl.java:89)
	at org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.write(WrappedMapper.java:112)
	at org.apache.hadoop.mapreduce.Mapper.map(Mapper.java:125)
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:146)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:799)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:347)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:178)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1845)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:172)
Caused by: org.apache.hadoop.util.DiskChecker$DiskErrorException: Could not find any valid local directory for attempt_1612828479085_0009_m_000221_1000_spill_0.out
	at org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.getLocalPathForWrite(LocalDirAllocator.java:462)
	at org.apache.hadoop.fs.LocalDirAllocator.getLocalPathForWrite(LocalDirAllocator.java:165)
	at org.apache.hadoop.fs.LocalDirAllocator.getLocalPathForWrite(LocalDirAllocator.java:146)
	at org.apache.hadoop.mapred.YarnOutputFiles.getSpillFileForWrite(YarnOutputFiles.java:157)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.sortAndSpill(MapTask.java:1617)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.access$1000(MapTask.java:888)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer$SpillThread.run(MapTask.java:1555)

2021-02-09 00:12:20,295 INFO mapreduce.Job: Task Id : attempt_1612828479085_0009_m_000218_1000, Status : FAILED
Error: java.io.IOException: Spill failed
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.checkSpillException(MapTask.java:1586)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.collect(MapTask.java:1099)
	at org.apache.hadoop.mapred.MapTask$NewOutputCollector.write(MapTask.java:727)
	at org.apache.hadoop.mapreduce.task.TaskInputOutputContextImpl.write(TaskInputOutputContextImpl.java:89)
	at org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.write(WrappedMapper.java:112)
	at org.apache.hadoop.mapreduce.Mapper.map(Mapper.java:125)
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:146)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:799)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:347)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:178)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1845)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:172)
Caused by: org.apache.hadoop.util.DiskChecker$DiskErrorException: Could not find any valid local directory for attempt_1612828479085_0009_m_000218_1000_spill_0.out
	at org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.getLocalPathForWrite(LocalDirAllocator.java:462)
	at org.apache.hadoop.fs.LocalDirAllocator.getLocalPathForWrite(LocalDirAllocator.java:165)
	at org.apache.hadoop.fs.LocalDirAllocator.getLocalPathForWrite(LocalDirAllocator.java:146)
	at org.apache.hadoop.mapred.YarnOutputFiles.getSpillFileForWrite(YarnOutputFiles.java:157)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.sortAndSpill(MapTask.java:1617)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.access$1000(MapTask.java:888)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer$SpillThread.run(MapTask.java:1555)

2021-02-09 00:12:20,296 INFO mapreduce.Job: Task Id : attempt_1612828479085_0009_m_000214_1000, Status : FAILED
Error: java.io.IOException: Spill failed
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.checkSpillException(MapTask.java:1586)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.collect(MapTask.java:1099)
	at org.apache.hadoop.mapred.MapTask$NewOutputCollector.write(MapTask.java:727)
	at org.apache.hadoop.mapreduce.task.TaskInputOutputContextImpl.write(TaskInputOutputContextImpl.java:89)
	at org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.write(WrappedMapper.java:112)
	at org.apache.hadoop.mapreduce.Mapper.map(Mapper.java:125)
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:146)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:799)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:347)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:178)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1845)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:172)
Caused by: org.apache.hadoop.util.DiskChecker$DiskErrorException: Could not find any valid local directory for attempt_1612828479085_0009_m_000214_1000_spill_0.out
	at org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.getLocalPathForWrite(LocalDirAllocator.java:462)
	at org.apache.hadoop.fs.LocalDirAllocator.getLocalPathForWrite(LocalDirAllocator.java:165)
	at org.apache.hadoop.fs.LocalDirAllocator.getLocalPathForWrite(LocalDirAllocator.java:146)
	at org.apache.hadoop.mapred.YarnOutputFiles.getSpillFileForWrite(YarnOutputFiles.java:157)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.sortAndSpill(MapTask.java:1617)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.access$1000(MapTask.java:888)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer$SpillThread.run(MapTask.java:1555)

2021-02-09 00:12:21,304 INFO mapreduce.Job: Task Id : attempt_1612828479085_0009_m_000224_1000, Status : FAILED
Error: java.io.IOException: Spill failed
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.checkSpillException(MapTask.java:1586)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.collect(MapTask.java:1099)
	at org.apache.hadoop.mapred.MapTask$NewOutputCollector.write(MapTask.java:727)
	at org.apache.hadoop.mapreduce.task.TaskInputOutputContextImpl.write(TaskInputOutputContextImpl.java:89)
	at org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.write(WrappedMapper.java:112)
	at org.apache.hadoop.mapreduce.Mapper.map(Mapper.java:125)
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:146)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:799)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:347)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:178)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1845)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:172)
Caused by: org.apache.hadoop.util.DiskChecker$DiskErrorException: Could not find any valid local directory for attempt_1612828479085_0009_m_000224_1000_spill_0.out
	at org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.getLocalPathForWrite(LocalDirAllocator.java:462)
	at org.apache.hadoop.fs.LocalDirAllocator.getLocalPathForWrite(LocalDirAllocator.java:165)
	at org.apache.hadoop.fs.LocalDirAllocator.getLocalPathForWrite(LocalDirAllocator.java:146)
	at org.apache.hadoop.mapred.YarnOutputFiles.getSpillFileForWrite(YarnOutputFiles.java:157)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.sortAndSpill(MapTask.java:1617)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.access$1000(MapTask.java:888)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer$SpillThread.run(MapTask.java:1555)

2021-02-09 00:12:21,309 INFO mapreduce.Job: Task Id : attempt_1612828479085_0009_m_000226_1000, Status : FAILED
Error: java.io.IOException: Spill failed
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.checkSpillException(MapTask.java:1586)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.collect(MapTask.java:1099)
	at org.apache.hadoop.mapred.MapTask$NewOutputCollector.write(MapTask.java:727)
	at org.apache.hadoop.mapreduce.task.TaskInputOutputContextImpl.write(TaskInputOutputContextImpl.java:89)
	at org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.write(WrappedMapper.java:112)
	at org.apache.hadoop.mapreduce.Mapper.map(Mapper.java:125)
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:146)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:799)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:347)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:178)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1845)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:172)
Caused by: org.apache.hadoop.util.DiskChecker$DiskErrorException: Could not find any valid local directory for attempt_1612828479085_0009_m_000226_1000_spill_0.out
	at org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.getLocalPathForWrite(LocalDirAllocator.java:462)
	at org.apache.hadoop.fs.LocalDirAllocator.getLocalPathForWrite(LocalDirAllocator.java:165)
	at org.apache.hadoop.fs.LocalDirAllocator.getLocalPathForWrite(LocalDirAllocator.java:146)
	at org.apache.hadoop.mapred.YarnOutputFiles.getSpillFileForWrite(YarnOutputFiles.java:157)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.sortAndSpill(MapTask.java:1617)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.access$1000(MapTask.java:888)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer$SpillThread.run(MapTask.java:1555)

2021-02-09 00:12:21,332 INFO mapreduce.Job: Task Id : attempt_1612828479085_0009_m_000234_1000, Status : FAILED
Error: java.io.IOException: Spill failed
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.checkSpillException(MapTask.java:1586)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.collect(MapTask.java:1099)
	at org.apache.hadoop.mapred.MapTask$NewOutputCollector.write(MapTask.java:727)
	at org.apache.hadoop.mapreduce.task.TaskInputOutputContextImpl.write(TaskInputOutputContextImpl.java:89)
	at org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.write(WrappedMapper.java:112)
	at org.apache.hadoop.mapreduce.Mapper.map(Mapper.java:125)
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:146)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:799)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:347)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:178)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1845)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:172)
Caused by: org.apache.hadoop.util.DiskChecker$DiskErrorException: Could not find any valid local directory for attempt_1612828479085_0009_m_000234_1000_spill_0.out
	at org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.getLocalPathForWrite(LocalDirAllocator.java:462)
	at org.apache.hadoop.fs.LocalDirAllocator.getLocalPathForWrite(LocalDirAllocator.java:165)
	at org.apache.hadoop.fs.LocalDirAllocator.getLocalPathForWrite(LocalDirAllocator.java:146)
	at org.apache.hadoop.mapred.YarnOutputFiles.getSpillFileForWrite(YarnOutputFiles.java:157)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.sortAndSpill(MapTask.java:1617)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.access$1000(MapTask.java:888)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer$SpillThread.run(MapTask.java:1555)

2021-02-09 00:12:22,356 INFO mapreduce.Job: Task Id : attempt_1612828479085_0009_m_000239_1000, Status : FAILED
Error: java.io.IOException: Spill failed
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.checkSpillException(MapTask.java:1586)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.collect(MapTask.java:1099)
	at org.apache.hadoop.mapred.MapTask$NewOutputCollector.write(MapTask.java:727)
	at org.apache.hadoop.mapreduce.task.TaskInputOutputContextImpl.write(TaskInputOutputContextImpl.java:89)
	at org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.write(WrappedMapper.java:112)
	at org.apache.hadoop.mapreduce.Mapper.map(Mapper.java:125)
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:146)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:799)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:347)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:178)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1845)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:172)
Caused by: org.apache.hadoop.util.DiskChecker$DiskErrorException: Could not find any valid local directory for attempt_1612828479085_0009_m_000239_1000_spill_0.out
	at org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.getLocalPathForWrite(LocalDirAllocator.java:462)
	at org.apache.hadoop.fs.LocalDirAllocator.getLocalPathForWrite(LocalDirAllocator.java:165)
	at org.apache.hadoop.fs.LocalDirAllocator.getLocalPathForWrite(LocalDirAllocator.java:146)
	at org.apache.hadoop.mapred.YarnOutputFiles.getSpillFileForWrite(YarnOutputFiles.java:157)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.sortAndSpill(MapTask.java:1617)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.access$1000(MapTask.java:888)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer$SpillThread.run(MapTask.java:1555)

2021-02-09 00:12:22,357 INFO mapreduce.Job: Task Id : attempt_1612828479085_0009_m_000228_1000, Status : FAILED
Error: java.io.IOException: Spill failed
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.checkSpillException(MapTask.java:1586)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.collect(MapTask.java:1099)
	at org.apache.hadoop.mapred.MapTask$NewOutputCollector.write(MapTask.java:727)
	at org.apache.hadoop.mapreduce.task.TaskInputOutputContextImpl.write(TaskInputOutputContextImpl.java:89)
	at org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.write(WrappedMapper.java:112)
	at org.apache.hadoop.mapreduce.Mapper.map(Mapper.java:125)
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:146)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:799)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:347)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:178)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1845)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:172)
Caused by: org.apache.hadoop.util.DiskChecker$DiskErrorException: Could not find any valid local directory for attempt_1612828479085_0009_m_000228_1000_spill_0.out
	at org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.getLocalPathForWrite(LocalDirAllocator.java:462)
	at org.apache.hadoop.fs.LocalDirAllocator.getLocalPathForWrite(LocalDirAllocator.java:165)
	at org.apache.hadoop.fs.LocalDirAllocator.getLocalPathForWrite(LocalDirAllocator.java:146)
	at org.apache.hadoop.mapred.YarnOutputFiles.getSpillFileForWrite(YarnOutputFiles.java:157)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.sortAndSpill(MapTask.java:1617)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.access$1000(MapTask.java:888)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer$SpillThread.run(MapTask.java:1555)

2021-02-09 00:12:23,372 INFO mapreduce.Job: Task Id : attempt_1612828479085_0009_m_000227_1000, Status : FAILED
Error: java.io.IOException: Spill failed
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.checkSpillException(MapTask.java:1586)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.collect(MapTask.java:1099)
	at org.apache.hadoop.mapred.MapTask$NewOutputCollector.write(MapTask.java:727)
	at org.apache.hadoop.mapreduce.task.TaskInputOutputContextImpl.write(TaskInputOutputContextImpl.java:89)
	at org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.write(WrappedMapper.java:112)
	at org.apache.hadoop.mapreduce.Mapper.map(Mapper.java:125)
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:146)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:799)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:347)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:178)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1845)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:172)
Caused by: org.apache.hadoop.util.DiskChecker$DiskErrorException: Could not find any valid local directory for attempt_1612828479085_0009_m_000227_1000_spill_0.out
	at org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.getLocalPathForWrite(LocalDirAllocator.java:462)
	at org.apache.hadoop.fs.LocalDirAllocator.getLocalPathForWrite(LocalDirAllocator.java:165)
	at org.apache.hadoop.fs.LocalDirAllocator.getLocalPathForWrite(LocalDirAllocator.java:146)
	at org.apache.hadoop.mapred.YarnOutputFiles.getSpillFileForWrite(YarnOutputFiles.java:157)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.sortAndSpill(MapTask.java:1617)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.access$1000(MapTask.java:888)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer$SpillThread.run(MapTask.java:1555)

2021-02-09 00:12:24,410 INFO mapreduce.Job: Task Id : attempt_1612828479085_0009_m_000240_1000, Status : FAILED
Error: java.io.IOException: Spill failed
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.checkSpillException(MapTask.java:1586)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.collect(MapTask.java:1099)
	at org.apache.hadoop.mapred.MapTask$NewOutputCollector.write(MapTask.java:727)
	at org.apache.hadoop.mapreduce.task.TaskInputOutputContextImpl.write(TaskInputOutputContextImpl.java:89)
	at org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.write(WrappedMapper.java:112)
	at org.apache.hadoop.mapreduce.Mapper.map(Mapper.java:125)
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:146)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:799)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:347)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:178)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1845)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:172)
Caused by: org.apache.hadoop.util.DiskChecker$DiskErrorException: Could not find any valid local directory for attempt_1612828479085_0009_m_000240_1000_spill_0.out
	at org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.getLocalPathForWrite(LocalDirAllocator.java:462)
	at org.apache.hadoop.fs.LocalDirAllocator.getLocalPathForWrite(LocalDirAllocator.java:165)
	at org.apache.hadoop.fs.LocalDirAllocator.getLocalPathForWrite(LocalDirAllocator.java:146)
	at org.apache.hadoop.mapred.YarnOutputFiles.getSpillFileForWrite(YarnOutputFiles.java:157)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.sortAndSpill(MapTask.java:1617)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.access$1000(MapTask.java:888)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer$SpillThread.run(MapTask.java:1555)

2021-02-09 00:12:25,415 INFO mapreduce.Job: Task Id : attempt_1612828479085_0009_m_000289_1000, Status : FAILED
[2021-02-09 00:12:23.911]Exception from container-launch.
Container id: container_1612828479085_0009_02_000098
Exit code: 1

[2021-02-09 00:12:23.913]Container exited with a non-zero exit code 1. Error file: prelaunch.err.
Last 4096 bytes of prelaunch.err :
ln: failed to create symbolic link 'job.xml': No space left on device

[2021-02-09 00:12:23.913]Container exited with a non-zero exit code 1. Error file: prelaunch.err.
Last 4096 bytes of prelaunch.err :
ln: failed to create symbolic link 'job.xml': No space left on device


2021-02-09 00:12:25,416 INFO mapreduce.Job: Task Id : attempt_1612828479085_0009_m_000241_1000, Status : FAILED
Error: java.io.IOException: Spill failed
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.checkSpillException(MapTask.java:1586)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.collect(MapTask.java:1099)
	at org.apache.hadoop.mapred.MapTask$NewOutputCollector.write(MapTask.java:727)
	at org.apache.hadoop.mapreduce.task.TaskInputOutputContextImpl.write(TaskInputOutputContextImpl.java:89)
	at org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.write(WrappedMapper.java:112)
	at org.apache.hadoop.mapreduce.Mapper.map(Mapper.java:125)
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:146)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:799)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:347)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:178)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1845)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:172)
Caused by: org.apache.hadoop.util.DiskChecker$DiskErrorException: Could not find any valid local directory for attempt_1612828479085_0009_m_000241_1000_spill_0.out
	at org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.getLocalPathForWrite(LocalDirAllocator.java:462)
	at org.apache.hadoop.fs.LocalDirAllocator.getLocalPathForWrite(LocalDirAllocator.java:165)
	at org.apache.hadoop.fs.LocalDirAllocator.getLocalPathForWrite(LocalDirAllocator.java:146)
	at org.apache.hadoop.mapred.YarnOutputFiles.getSpillFileForWrite(YarnOutputFiles.java:157)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.sortAndSpill(MapTask.java:1617)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.access$1000(MapTask.java:888)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer$SpillThread.run(MapTask.java:1555)

2021-02-09 00:12:26,424 INFO mapreduce.Job: Task Id : attempt_1612828479085_0009_m_000242_1000, Status : FAILED
Error: java.io.IOException: Spill failed
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.checkSpillException(MapTask.java:1586)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.collect(MapTask.java:1099)
	at org.apache.hadoop.mapred.MapTask$NewOutputCollector.write(MapTask.java:727)
	at org.apache.hadoop.mapreduce.task.TaskInputOutputContextImpl.write(TaskInputOutputContextImpl.java:89)
	at org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.write(WrappedMapper.java:112)
	at org.apache.hadoop.mapreduce.Mapper.map(Mapper.java:125)
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:146)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:799)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:347)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:178)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1845)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:172)
Caused by: org.apache.hadoop.util.DiskChecker$DiskErrorException: Could not find any valid local directory for attempt_1612828479085_0009_m_000242_1000_spill_0.out
	at org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.getLocalPathForWrite(LocalDirAllocator.java:462)
	at org.apache.hadoop.fs.LocalDirAllocator.getLocalPathForWrite(LocalDirAllocator.java:165)
	at org.apache.hadoop.fs.LocalDirAllocator.getLocalPathForWrite(LocalDirAllocator.java:146)
	at org.apache.hadoop.mapred.YarnOutputFiles.getSpillFileForWrite(YarnOutputFiles.java:157)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.sortAndSpill(MapTask.java:1617)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.access$1000(MapTask.java:888)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer$SpillThread.run(MapTask.java:1555)

2021-02-09 00:12:26,426 INFO mapreduce.Job: Task Id : attempt_1612828479085_0009_m_000285_1000, Status : FAILED
Error: java.io.FileNotFoundException: /usr/lib/hadoop/tmp/nm-local-dir/usercache/ec2-user/appcache/application_1612828479085_0009/container_1612828479085_0009_02_000095/.job.xml.crc (No space left on device)
	at java.io.FileOutputStream.open0(Native Method)
	at java.io.FileOutputStream.open(FileOutputStream.java:270)
	at java.io.FileOutputStream.<init>(FileOutputStream.java:213)
	at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:251)
	at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:234)
	at org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:333)
	at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:322)
	at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:353)
	at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:420)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:479)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:458)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1164)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1144)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1033)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1021)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:697)
	at org.apache.hadoop.mapred.YarnChild.writeLocalJobFile(YarnChild.java:373)
	at org.apache.hadoop.mapred.YarnChild.configureTask(YarnChild.java:355)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:150)

2021-02-09 00:12:27,438 INFO mapreduce.Job: Task Id : attempt_1612828479085_0009_m_000244_1000, Status : FAILED
Error: java.io.IOException: Spill failed
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.checkSpillException(MapTask.java:1586)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.collect(MapTask.java:1099)
	at org.apache.hadoop.mapred.MapTask$NewOutputCollector.write(MapTask.java:727)
	at org.apache.hadoop.mapreduce.task.TaskInputOutputContextImpl.write(TaskInputOutputContextImpl.java:89)
	at org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.write(WrappedMapper.java:112)
	at org.apache.hadoop.mapreduce.Mapper.map(Mapper.java:125)
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:146)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:799)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:347)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:178)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1845)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:172)
Caused by: org.apache.hadoop.util.DiskChecker$DiskErrorException: Could not find any valid local directory for attempt_1612828479085_0009_m_000244_1000_spill_0.out
	at org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.getLocalPathForWrite(LocalDirAllocator.java:462)
	at org.apache.hadoop.fs.LocalDirAllocator.getLocalPathForWrite(LocalDirAllocator.java:165)
	at org.apache.hadoop.fs.LocalDirAllocator.getLocalPathForWrite(LocalDirAllocator.java:146)
	at org.apache.hadoop.mapred.YarnOutputFiles.getSpillFileForWrite(YarnOutputFiles.java:157)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.sortAndSpill(MapTask.java:1617)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.access$1000(MapTask.java:888)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer$SpillThread.run(MapTask.java:1555)

2021-02-09 00:12:27,441 INFO mapreduce.Job: Task Id : attempt_1612828479085_0009_m_000286_1000, Status : FAILED
FSError: java.io.IOException: No space left on device
2021-02-09 00:12:27,452 INFO mapreduce.Job: Task Id : attempt_1612828479085_0009_m_000298_1000, Status : FAILED
[2021-02-09 00:12:25.481]/usr/lib/hadoop/tmp/nm-local-dir/usercache/ec2-user/appcache/application_1612828479085_0009/container_1612828479085_0009_02_000104/launch_container.sh (No space left on device)
[2021-02-09 00:12:25.482]/usr/lib/hadoop/tmp/nm-local-dir/usercache/ec2-user/appcache/application_1612828479085_0009/container_1612828479085_0009_02_000104/launch_container.sh (No space left on device)

2021-02-09 00:12:27,453 INFO mapreduce.Job: Task Id : attempt_1612828479085_0009_m_000246_1000, Status : FAILED
Error: java.io.IOException: Spill failed
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.checkSpillException(MapTask.java:1586)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.collect(MapTask.java:1099)
	at org.apache.hadoop.mapred.MapTask$NewOutputCollector.write(MapTask.java:727)
	at org.apache.hadoop.mapreduce.task.TaskInputOutputContextImpl.write(TaskInputOutputContextImpl.java:89)
	at org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.write(WrappedMapper.java:112)
	at org.apache.hadoop.mapreduce.Mapper.map(Mapper.java:125)
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:146)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:799)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:347)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:178)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1845)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:172)
Caused by: org.apache.hadoop.util.DiskChecker$DiskErrorException: Could not find any valid local directory for attempt_1612828479085_0009_m_000246_1000_spill_0.out
	at org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.getLocalPathForWrite(LocalDirAllocator.java:462)
	at org.apache.hadoop.fs.LocalDirAllocator.getLocalPathForWrite(LocalDirAllocator.java:165)
	at org.apache.hadoop.fs.LocalDirAllocator.getLocalPathForWrite(LocalDirAllocator.java:146)
	at org.apache.hadoop.mapred.YarnOutputFiles.getSpillFileForWrite(YarnOutputFiles.java:157)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.sortAndSpill(MapTask.java:1617)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.access$1000(MapTask.java:888)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer$SpillThread.run(MapTask.java:1555)

2021-02-09 00:12:27,455 INFO mapreduce.Job: Task Id : attempt_1612828479085_0009_m_000250_1000, Status : FAILED
Error: java.io.IOException: Spill failed
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.checkSpillException(MapTask.java:1586)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.collect(MapTask.java:1099)
	at org.apache.hadoop.mapred.MapTask$NewOutputCollector.write(MapTask.java:727)
	at org.apache.hadoop.mapreduce.task.TaskInputOutputContextImpl.write(TaskInputOutputContextImpl.java:89)
	at org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.write(WrappedMapper.java:112)
	at org.apache.hadoop.mapreduce.Mapper.map(Mapper.java:125)
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:146)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:799)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:347)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:178)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1845)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:172)
Caused by: org.apache.hadoop.util.DiskChecker$DiskErrorException: Could not find any valid local directory for attempt_1612828479085_0009_m_000250_1000_spill_0.out
	at org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.getLocalPathForWrite(LocalDirAllocator.java:462)
	at org.apache.hadoop.fs.LocalDirAllocator.getLocalPathForWrite(LocalDirAllocator.java:165)
	at org.apache.hadoop.fs.LocalDirAllocator.getLocalPathForWrite(LocalDirAllocator.java:146)
	at org.apache.hadoop.mapred.YarnOutputFiles.getSpillFileForWrite(YarnOutputFiles.java:157)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.sortAndSpill(MapTask.java:1617)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.access$1000(MapTask.java:888)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer$SpillThread.run(MapTask.java:1555)

2021-02-09 00:12:28,459 INFO mapreduce.Job: Task Id : attempt_1612828479085_0009_m_000299_1000, Status : FAILED
[2021-02-09 00:12:26.611]/usr/lib/hadoop/tmp/nm-local-dir/usercache/ec2-user/appcache/application_1612828479085_0009/container_1612828479085_0009_02_000105/default_container_executor.sh (No space left on device)
[2021-02-09 00:12:26.611]/usr/lib/hadoop/tmp/nm-local-dir/usercache/ec2-user/appcache/application_1612828479085_0009/container_1612828479085_0009_02_000105/default_container_executor.sh (No space left on device)

2021-02-09 00:12:28,460 INFO mapreduce.Job: Task Id : attempt_1612828479085_0009_m_000301_1000, Status : FAILED
[2021-02-09 00:12:26.796]Exception from container-launch.
Container id: container_1612828479085_0009_02_000106
Exit code: 1
Exception message: /usr/lib/hadoop/tmp/nm-local-dir/usercache/ec2-user/appcache/application_1612828479085_0009/container_1612828479085_0009_02_000106/default_container_executor_session.sh: line 3: /usr/lib/hadoop/tmp/nm-local-dir/nmPrivate/application_1612828479085_0009/container_1612828479085_0009_02_000106/container_1612828479085_0009_02_000106.pid.tmp: No space left on device
/bin/mv: cannot stat '/usr/lib/hadoop/tmp/nm-local-dir/nmPrivate/application_1612828479085_0009/container_1612828479085_0009_02_000106/container_1612828479085_0009_02_000106.pid.tmp': No such file or directory
/usr/lib/hadoop/tmp/nm-local-dir/usercache/ec2-user/appcache/application_1612828479085_0009/container_1612828479085_0009_02_000106/launch_container.sh: line 5: /usr/lib/hadoop/logs/userlogs/application_1612828479085_0009/container_1612828479085_0009_02_000106/prelaunch.out: No space left on device
/usr/lib/hadoop/tmp/nm-local-dir/usercache/ec2-user/appcache/application_1612828479085_0009/container_1612828479085_0009_02_000106/default_container_executor.sh: line 4: /usr/lib/hadoop/tmp/nm-local-dir/nmPrivate/application_1612828479085_0009/container_1612828479085_0009_02_000106/container_1612828479085_0009_02_000106.pid.exitcode.tmp: No space left on device
/bin/mv: cannot stat '/usr/lib/hadoop/tmp/nm-local-dir/nmPrivate/application_1612828479085_0009/container_1612828479085_0009_02_000106/container_1612828479085_0009_02_000106.pid.exitcode.tmp': No such file or directory


[2021-02-09 00:12:26.797]Container exited with a non-zero exit code 1. 
[2021-02-09 00:12:26.803]Container exited with a non-zero exit code 1. 

2021-02-09 00:12:28,460 INFO mapreduce.Job: Task Id : attempt_1612828479085_0009_m_000271_1000, Status : FAILED
Error: java.io.IOException: Spill failed
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.checkSpillException(MapTask.java:1586)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.collect(MapTask.java:1099)
	at org.apache.hadoop.mapred.MapTask$NewOutputCollector.write(MapTask.java:727)
	at org.apache.hadoop.mapreduce.task.TaskInputOutputContextImpl.write(TaskInputOutputContextImpl.java:89)
	at org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.write(WrappedMapper.java:112)
	at org.apache.hadoop.mapreduce.Mapper.map(Mapper.java:125)
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:146)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:799)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:347)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:178)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1845)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:172)
Caused by: org.apache.hadoop.util.DiskChecker$DiskErrorException: Could not find any valid local directory for attempt_1612828479085_0009_m_000271_1000_spill_0.out
	at org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.getLocalPathForWrite(LocalDirAllocator.java:462)
	at org.apache.hadoop.fs.LocalDirAllocator.getLocalPathForWrite(LocalDirAllocator.java:165)
	at org.apache.hadoop.fs.LocalDirAllocator.getLocalPathForWrite(LocalDirAllocator.java:146)
	at org.apache.hadoop.mapred.YarnOutputFiles.getSpillFileForWrite(YarnOutputFiles.java:157)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.sortAndSpill(MapTask.java:1617)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.access$1000(MapTask.java:888)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer$SpillThread.run(MapTask.java:1555)

2021-02-09 00:12:28,461 INFO mapreduce.Job: Task Id : attempt_1612828479085_0009_m_000248_1000, Status : FAILED
Error: java.io.IOException: Spill failed
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.checkSpillException(MapTask.java:1586)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.collect(MapTask.java:1099)
	at org.apache.hadoop.mapred.MapTask$NewOutputCollector.write(MapTask.java:727)
	at org.apache.hadoop.mapreduce.task.TaskInputOutputContextImpl.write(TaskInputOutputContextImpl.java:89)
	at org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.write(WrappedMapper.java:112)
	at org.apache.hadoop.mapreduce.Mapper.map(Mapper.java:125)
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:146)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:799)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:347)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:178)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1845)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:172)
Caused by: org.apache.hadoop.util.DiskChecker$DiskErrorException: Could not find any valid local directory for attempt_1612828479085_0009_m_000248_1000_spill_0.out
	at org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.getLocalPathForWrite(LocalDirAllocator.java:462)
	at org.apache.hadoop.fs.LocalDirAllocator.getLocalPathForWrite(LocalDirAllocator.java:165)
	at org.apache.hadoop.fs.LocalDirAllocator.getLocalPathForWrite(LocalDirAllocator.java:146)
	at org.apache.hadoop.mapred.YarnOutputFiles.getSpillFileForWrite(YarnOutputFiles.java:157)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.sortAndSpill(MapTask.java:1617)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.access$1000(MapTask.java:888)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer$SpillThread.run(MapTask.java:1555)

2021-02-09 00:12:28,462 INFO mapreduce.Job: Task Id : attempt_1612828479085_0009_m_000276_1000, Status : FAILED
Error: java.io.IOException: Spill failed
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.checkSpillException(MapTask.java:1586)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.collect(MapTask.java:1099)
	at org.apache.hadoop.mapred.MapTask$NewOutputCollector.write(MapTask.java:727)
	at org.apache.hadoop.mapreduce.task.TaskInputOutputContextImpl.write(TaskInputOutputContextImpl.java:89)
	at org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.write(WrappedMapper.java:112)
	at org.apache.hadoop.mapreduce.Mapper.map(Mapper.java:125)
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:146)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:799)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:347)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:178)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1845)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:172)
Caused by: org.apache.hadoop.util.DiskChecker$DiskErrorException: Could not find any valid local directory for attempt_1612828479085_0009_m_000276_1000_spill_0.out
	at org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.getLocalPathForWrite(LocalDirAllocator.java:462)
	at org.apache.hadoop.fs.LocalDirAllocator.getLocalPathForWrite(LocalDirAllocator.java:165)
	at org.apache.hadoop.fs.LocalDirAllocator.getLocalPathForWrite(LocalDirAllocator.java:146)
	at org.apache.hadoop.mapred.YarnOutputFiles.getSpillFileForWrite(YarnOutputFiles.java:157)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.sortAndSpill(MapTask.java:1617)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.access$1000(MapTask.java:888)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer$SpillThread.run(MapTask.java:1555)

2021-02-09 00:12:28,462 INFO mapreduce.Job: Task Id : attempt_1612828479085_0009_m_000269_1000, Status : FAILED
Error: java.io.IOException: Spill failed
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.checkSpillException(MapTask.java:1586)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.collect(MapTask.java:1099)
	at org.apache.hadoop.mapred.MapTask$NewOutputCollector.write(MapTask.java:727)
	at org.apache.hadoop.mapreduce.task.TaskInputOutputContextImpl.write(TaskInputOutputContextImpl.java:89)
	at org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.write(WrappedMapper.java:112)
	at org.apache.hadoop.mapreduce.Mapper.map(Mapper.java:125)
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:146)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:799)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:347)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:178)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1845)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:172)
Caused by: org.apache.hadoop.util.DiskChecker$DiskErrorException: Could not find any valid local directory for attempt_1612828479085_0009_m_000269_1000_spill_0.out
	at org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.getLocalPathForWrite(LocalDirAllocator.java:462)
	at org.apache.hadoop.fs.LocalDirAllocator.getLocalPathForWrite(LocalDirAllocator.java:165)
	at org.apache.hadoop.fs.LocalDirAllocator.getLocalPathForWrite(LocalDirAllocator.java:146)
	at org.apache.hadoop.mapred.YarnOutputFiles.getSpillFileForWrite(YarnOutputFiles.java:157)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.sortAndSpill(MapTask.java:1617)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.access$1000(MapTask.java:888)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer$SpillThread.run(MapTask.java:1555)

2021-02-09 00:12:29,524 INFO mapreduce.Job: Task Id : attempt_1612828479085_0009_m_000282_1000, Status : FAILED
Error: java.io.IOException: Spill failed
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.checkSpillException(MapTask.java:1586)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.collect(MapTask.java:1099)
	at org.apache.hadoop.mapred.MapTask$NewOutputCollector.write(MapTask.java:727)
	at org.apache.hadoop.mapreduce.task.TaskInputOutputContextImpl.write(TaskInputOutputContextImpl.java:89)
	at org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.write(WrappedMapper.java:112)
	at org.apache.hadoop.mapreduce.Mapper.map(Mapper.java:125)
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:146)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:799)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:347)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:178)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1845)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:172)
Caused by: org.apache.hadoop.util.DiskChecker$DiskErrorException: Could not find any valid local directory for attempt_1612828479085_0009_m_000282_1000_spill_0.out
	at org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.getLocalPathForWrite(LocalDirAllocator.java:462)
	at org.apache.hadoop.fs.LocalDirAllocator.getLocalPathForWrite(LocalDirAllocator.java:165)
	at org.apache.hadoop.fs.LocalDirAllocator.getLocalPathForWrite(LocalDirAllocator.java:146)
	at org.apache.hadoop.mapred.YarnOutputFiles.getSpillFileForWrite(YarnOutputFiles.java:157)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.sortAndSpill(MapTask.java:1617)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.access$1000(MapTask.java:888)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer$SpillThread.run(MapTask.java:1555)

2021-02-09 00:12:29,542 INFO mapreduce.Job: Task Id : attempt_1612828479085_0009_m_000343_1000, Status : FAILED
[2021-02-09 00:12:28.699]Exception from container-launch.
Container id: container_1612828479085_0009_02_000115
Exit code: 1

[2021-02-09 00:12:28.700]Container exited with a non-zero exit code 1. Error file: prelaunch.err.
Last 4096 bytes of prelaunch.err :
ln: failed to create symbolic link 'job.xml': No space left on device

[2021-02-09 00:12:28.703]Container exited with a non-zero exit code 1. Error file: prelaunch.err.
Last 4096 bytes of prelaunch.err :
ln: failed to create symbolic link 'job.xml': No space left on device


2021-02-09 00:12:29,547 INFO mapreduce.Job: Task Id : attempt_1612828479085_0009_m_000342_1000, Status : FAILED
[2021-02-09 00:12:28.708]Exception from container-launch.
Container id: container_1612828479085_0009_02_000114
Exit code: 1

[2021-02-09 00:12:28.710]Container exited with a non-zero exit code 1. Error file: prelaunch.err.
Last 4096 bytes of prelaunch.err :
ln: failed to create symbolic link '_partition.lst': No space left on device

[2021-02-09 00:12:28.710]Container exited with a non-zero exit code 1. Error file: prelaunch.err.
Last 4096 bytes of prelaunch.err :
ln: failed to create symbolic link '_partition.lst': No space left on device


2021-02-09 00:12:30,592 INFO mapreduce.Job: Task Id : attempt_1612828479085_0009_m_000280_1000, Status : FAILED
Error: java.io.IOException: Spill failed
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.checkSpillException(MapTask.java:1586)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.collect(MapTask.java:1099)
	at org.apache.hadoop.mapred.MapTask$NewOutputCollector.write(MapTask.java:727)
	at org.apache.hadoop.mapreduce.task.TaskInputOutputContextImpl.write(TaskInputOutputContextImpl.java:89)
	at org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.write(WrappedMapper.java:112)
	at org.apache.hadoop.mapreduce.Mapper.map(Mapper.java:125)
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:146)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:799)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:347)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:178)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1845)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:172)
Caused by: org.apache.hadoop.util.DiskChecker$DiskErrorException: Could not find any valid local directory for attempt_1612828479085_0009_m_000280_1000_spill_0.out
	at org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.getLocalPathForWrite(LocalDirAllocator.java:462)
	at org.apache.hadoop.fs.LocalDirAllocator.getLocalPathForWrite(LocalDirAllocator.java:165)
	at org.apache.hadoop.fs.LocalDirAllocator.getLocalPathForWrite(LocalDirAllocator.java:146)
	at org.apache.hadoop.mapred.YarnOutputFiles.getSpillFileForWrite(YarnOutputFiles.java:157)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.sortAndSpill(MapTask.java:1617)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.access$1000(MapTask.java:888)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer$SpillThread.run(MapTask.java:1555)

2021-02-09 00:12:30,595 INFO mapreduce.Job: Task Id : attempt_1612828479085_0009_m_000346_1000, Status : FAILED
[2021-02-09 00:12:29.899]Exception from container-launch.
Container id: container_1612828479085_0009_02_000117
Exit code: 1

[2021-02-09 00:12:29.901]Container exited with a non-zero exit code 1. Error file: prelaunch.err.
Last 4096 bytes of prelaunch.err :
ln: failed to create symbolic link '_partition.lst': No space left on device

[2021-02-09 00:12:29.917]Container exited with a non-zero exit code 1. Error file: prelaunch.err.
Last 4096 bytes of prelaunch.err :
ln: failed to create symbolic link '_partition.lst': No space left on device


2021-02-09 00:12:30,598 INFO mapreduce.Job: Task Id : attempt_1612828479085_0009_m_000347_1000, Status : FAILED
[2021-02-09 00:12:29.941]Exception from container-launch.
Container id: container_1612828479085_0009_02_000118
Exit code: 1
Exception message: /usr/lib/hadoop/tmp/nm-local-dir/usercache/ec2-user/appcache/application_1612828479085_0009/container_1612828479085_0009_02_000118/launch_container.sh: line 5: /usr/lib/hadoop/logs/userlogs/application_1612828479085_0009/container_1612828479085_0009_02_000118/prelaunch.out: No space left on device


[2021-02-09 00:12:29.942]Container exited with a non-zero exit code 1. 
[2021-02-09 00:12:29.944]Container exited with a non-zero exit code 1. 

2021-02-09 00:12:30,611 INFO mapreduce.Job: Task Id : attempt_1612828479085_0009_m_000349_1000, Status : FAILED
[2021-02-09 00:12:29.946]Exception from container-launch.
Container id: container_1612828479085_0009_02_000119
Exit code: 1
Exception message: /usr/lib/hadoop/tmp/nm-local-dir/usercache/ec2-user/appcache/application_1612828479085_0009/container_1612828479085_0009_02_000119/launch_container.sh: line 5: /usr/lib/hadoop/logs/userlogs/application_1612828479085_0009/container_1612828479085_0009_02_000119/prelaunch.out: No space left on device


[2021-02-09 00:12:29.947]Container exited with a non-zero exit code 1. 
[2021-02-09 00:12:29.962]Container exited with a non-zero exit code 1. 

2021-02-09 00:12:31,645 INFO mapreduce.Job: Task Id : attempt_1612828479085_0009_m_000283_1000, Status : FAILED
Error: java.io.IOException: Spill failed
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.checkSpillException(MapTask.java:1586)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.collect(MapTask.java:1099)
	at org.apache.hadoop.mapred.MapTask$NewOutputCollector.write(MapTask.java:727)
	at org.apache.hadoop.mapreduce.task.TaskInputOutputContextImpl.write(TaskInputOutputContextImpl.java:89)
	at org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.write(WrappedMapper.java:112)
	at org.apache.hadoop.mapreduce.Mapper.map(Mapper.java:125)
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:146)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:799)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:347)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:178)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1845)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:172)
Caused by: org.apache.hadoop.util.DiskChecker$DiskErrorException: Could not find any valid local directory for attempt_1612828479085_0009_m_000283_1000_spill_0.out
	at org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.getLocalPathForWrite(LocalDirAllocator.java:462)
	at org.apache.hadoop.fs.LocalDirAllocator.getLocalPathForWrite(LocalDirAllocator.java:165)
	at org.apache.hadoop.fs.LocalDirAllocator.getLocalPathForWrite(LocalDirAllocator.java:146)
	at org.apache.hadoop.mapred.YarnOutputFiles.getSpillFileForWrite(YarnOutputFiles.java:157)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.sortAndSpill(MapTask.java:1617)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.access$1000(MapTask.java:888)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer$SpillThread.run(MapTask.java:1555)

2021-02-09 00:12:31,646 INFO mapreduce.Job: Task Id : attempt_1612828479085_0009_m_000353_1000, Status : FAILED
[2021-02-09 00:12:30.786]/usr/lib/hadoop/tmp/nm-local-dir/usercache/ec2-user/appcache/application_1612828479085_0009/container_1612828479085_0009_02_000122/default_container_executor.sh (No space left on device)
[2021-02-09 00:12:30.786]/usr/lib/hadoop/tmp/nm-local-dir/usercache/ec2-user/appcache/application_1612828479085_0009/container_1612828479085_0009_02_000122/default_container_executor.sh (No space left on device)

2021-02-09 00:12:31,646 INFO mapreduce.Job: Task Id : attempt_1612828479085_0009_m_000352_1000, Status : FAILED
[2021-02-09 00:12:30.877]Exception from container-launch.
Container id: container_1612828479085_0009_02_000121
Exit code: 1
Exception message: /usr/lib/hadoop/tmp/nm-local-dir/usercache/ec2-user/appcache/application_1612828479085_0009/container_1612828479085_0009_02_000121/default_container_executor_session.sh: line 3: /usr/lib/hadoop/tmp/nm-local-dir/nmPrivate/application_1612828479085_0009/container_1612828479085_0009_02_000121/container_1612828479085_0009_02_000121.pid.tmp: No space left on device
/bin/mv: cannot stat '/usr/lib/hadoop/tmp/nm-local-dir/nmPrivate/application_1612828479085_0009/container_1612828479085_0009_02_000121/container_1612828479085_0009_02_000121.pid.tmp': No such file or directory
/bin/mv: cannot move '/usr/lib/hadoop/tmp/nm-local-dir/nmPrivate/application_1612828479085_0009/container_1612828479085_0009_02_000121/container_1612828479085_0009_02_000121.pid.exitcode.tmp' to '/usr/lib/hadoop/tmp/nm-local-dir/nmPrivate/application_1612828479085_0009/container_1612828479085_0009_02_000121/container_1612828479085_0009_02_000121.pid.exitcode': No space left on device


[2021-02-09 00:12:30.883]Container exited with a non-zero exit code 1. Error file: prelaunch.err.
Last 4096 bytes of prelaunch.err :
ln: failed to create symbolic link 'job.jar': No space left on device

[2021-02-09 00:12:30.883]Container exited with a non-zero exit code 1. Error file: prelaunch.err.
Last 4096 bytes of prelaunch.err :
ln: failed to create symbolic link 'job.jar': No space left on device


2021-02-09 00:12:32,662 INFO mapreduce.Job: Task Id : attempt_1612828479085_0009_m_000293_1000, Status : FAILED
Error: java.io.IOException: Spill failed
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.checkSpillException(MapTask.java:1586)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.collect(MapTask.java:1099)
	at org.apache.hadoop.mapred.MapTask$NewOutputCollector.write(MapTask.java:727)
	at org.apache.hadoop.mapreduce.task.TaskInputOutputContextImpl.write(TaskInputOutputContextImpl.java:89)
	at org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.write(WrappedMapper.java:112)
	at org.apache.hadoop.mapreduce.Mapper.map(Mapper.java:125)
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:146)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:799)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:347)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:178)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1845)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:172)
Caused by: org.apache.hadoop.util.DiskChecker$DiskErrorException: Could not find any valid local directory for attempt_1612828479085_0009_m_000293_1000_spill_0.out
	at org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.getLocalPathForWrite(LocalDirAllocator.java:462)
	at org.apache.hadoop.fs.LocalDirAllocator.getLocalPathForWrite(LocalDirAllocator.java:165)
	at org.apache.hadoop.fs.LocalDirAllocator.getLocalPathForWrite(LocalDirAllocator.java:146)
	at org.apache.hadoop.mapred.YarnOutputFiles.getSpillFileForWrite(YarnOutputFiles.java:157)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.sortAndSpill(MapTask.java:1617)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.access$1000(MapTask.java:888)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer$SpillThread.run(MapTask.java:1555)

2021-02-09 00:12:32,676 INFO mapreduce.Job: Task Id : attempt_1612828479085_0009_m_000344_1000, Status : FAILED
Error: java.io.FileNotFoundException: /usr/lib/hadoop/tmp/nm-local-dir/usercache/ec2-user/appcache/application_1612828479085_0009/container_1612828479085_0009_02_000116/job.xml (No space left on device)
	at java.io.FileOutputStream.open0(Native Method)
	at java.io.FileOutputStream.open(FileOutputStream.java:270)
	at java.io.FileOutputStream.<init>(FileOutputStream.java:213)
	at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:251)
	at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:234)
	at org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:333)
	at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:322)
	at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:353)
	at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:416)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:479)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:458)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1164)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1144)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1033)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1021)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:697)
	at org.apache.hadoop.mapred.YarnChild.writeLocalJobFile(YarnChild.java:373)
	at org.apache.hadoop.mapred.YarnChild.configureTask(YarnChild.java:355)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:150)

2021-02-09 00:12:32,702 INFO mapreduce.Job: Task Id : attempt_1612828479085_0009_m_000335_1000, Status : FAILED
Error: java.io.FileNotFoundException: /usr/lib/hadoop/tmp/nm-local-dir/usercache/ec2-user/appcache/application_1612828479085_0009/container_1612828479085_0009_02_000111/job.xml (No space left on device)
	at java.io.FileOutputStream.open0(Native Method)
	at java.io.FileOutputStream.open(FileOutputStream.java:270)
	at java.io.FileOutputStream.<init>(FileOutputStream.java:213)
	at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:251)
	at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:234)
	at org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:333)
	at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:322)
	at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:353)
	at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:416)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:479)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:458)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1164)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1144)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1033)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1021)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:697)
	at org.apache.hadoop.mapred.YarnChild.writeLocalJobFile(YarnChild.java:373)
	at org.apache.hadoop.mapred.YarnChild.configureTask(YarnChild.java:355)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:150)

2021-02-09 00:12:32,703 INFO mapreduce.Job: Task Id : attempt_1612828479085_0009_m_000320_1000, Status : FAILED
FSError: java.io.IOException: No space left on device
2021-02-09 00:12:32,704 INFO mapreduce.Job: Task Id : attempt_1612828479085_0009_m_000321_1000, Status : FAILED
FSError: java.io.IOException: No space left on device
2021-02-09 00:12:32,704 INFO mapreduce.Job: Task Id : attempt_1612828479085_0009_m_000334_1000, Status : FAILED
Error: java.io.FileNotFoundException: /usr/lib/hadoop/tmp/nm-local-dir/usercache/ec2-user/appcache/application_1612828479085_0009/container_1612828479085_0009_02_000110/job.xml (No space left on device)
	at java.io.FileOutputStream.open0(Native Method)
	at java.io.FileOutputStream.open(FileOutputStream.java:270)
	at java.io.FileOutputStream.<init>(FileOutputStream.java:213)
	at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:251)
	at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:234)
	at org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:333)
	at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:322)
	at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:353)
	at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:416)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:479)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:458)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1164)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1144)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1033)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1021)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:697)
	at org.apache.hadoop.mapred.YarnChild.writeLocalJobFile(YarnChild.java:373)
	at org.apache.hadoop.mapred.YarnChild.configureTask(YarnChild.java:355)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:150)

2021-02-09 00:12:32,705 INFO mapreduce.Job: Task Id : attempt_1612828479085_0009_m_000325_1000, Status : FAILED
FSError: java.io.IOException: No space left on device
2021-02-09 00:12:32,706 INFO mapreduce.Job: Task Id : attempt_1612828479085_0009_m_000296_1000, Status : FAILED
Error: java.io.IOException: Spill failed
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.checkSpillException(MapTask.java:1586)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.collect(MapTask.java:1099)
	at org.apache.hadoop.mapred.MapTask$NewOutputCollector.write(MapTask.java:727)
	at org.apache.hadoop.mapreduce.task.TaskInputOutputContextImpl.write(TaskInputOutputContextImpl.java:89)
	at org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.write(WrappedMapper.java:112)
	at org.apache.hadoop.mapreduce.Mapper.map(Mapper.java:125)
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:146)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:799)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:347)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:178)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1845)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:172)
Caused by: org.apache.hadoop.util.DiskChecker$DiskErrorException: Could not find any valid local directory for attempt_1612828479085_0009_m_000296_1000_spill_0.out
	at org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.getLocalPathForWrite(LocalDirAllocator.java:462)
	at org.apache.hadoop.fs.LocalDirAllocator.getLocalPathForWrite(LocalDirAllocator.java:165)
	at org.apache.hadoop.fs.LocalDirAllocator.getLocalPathForWrite(LocalDirAllocator.java:146)
	at org.apache.hadoop.mapred.YarnOutputFiles.getSpillFileForWrite(YarnOutputFiles.java:157)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.sortAndSpill(MapTask.java:1617)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.access$1000(MapTask.java:888)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer$SpillThread.run(MapTask.java:1555)

2021-02-09 00:12:32,706 INFO mapreduce.Job: Task Id : attempt_1612828479085_0009_m_000288_1000, Status : FAILED
Error: java.io.IOException: Spill failed
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.checkSpillException(MapTask.java:1586)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.collect(MapTask.java:1099)
	at org.apache.hadoop.mapred.MapTask$NewOutputCollector.write(MapTask.java:727)
	at org.apache.hadoop.mapreduce.task.TaskInputOutputContextImpl.write(TaskInputOutputContextImpl.java:89)
	at org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.write(WrappedMapper.java:112)
	at org.apache.hadoop.mapreduce.Mapper.map(Mapper.java:125)
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:146)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:799)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:347)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:178)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1845)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:172)
Caused by: org.apache.hadoop.util.DiskChecker$DiskErrorException: Could not find any valid local directory for attempt_1612828479085_0009_m_000288_1000_spill_0.out
	at org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.getLocalPathForWrite(LocalDirAllocator.java:462)
	at org.apache.hadoop.fs.LocalDirAllocator.getLocalPathForWrite(LocalDirAllocator.java:165)
	at org.apache.hadoop.fs.LocalDirAllocator.getLocalPathForWrite(LocalDirAllocator.java:146)
	at org.apache.hadoop.mapred.YarnOutputFiles.getSpillFileForWrite(YarnOutputFiles.java:157)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.sortAndSpill(MapTask.java:1617)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.access$1000(MapTask.java:888)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer$SpillThread.run(MapTask.java:1555)

2021-02-09 00:12:32,707 INFO mapreduce.Job: Task Id : attempt_1612828479085_0009_m_000374_1000, Status : FAILED
[2021-02-09 00:12:31.822]Exception from container-launch.
Container id: container_1612828479085_0009_02_000126
Exit code: 1

[2021-02-09 00:12:31.823]Container exited with a non-zero exit code 1. Error file: prelaunch.err.
Last 4096 bytes of prelaunch.err :
ln: failed to create symbolic link 'job.xml': No space left on device

[2021-02-09 00:12:31.828]Container exited with a non-zero exit code 1. Error file: prelaunch.err.
Last 4096 bytes of prelaunch.err :
ln: failed to create symbolic link 'job.xml': No space left on device


2021-02-09 00:12:33,711 INFO mapreduce.Job: Task Id : attempt_1612828479085_0009_m_000294_1000, Status : FAILED
Error: java.io.IOException: Spill failed
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.checkSpillException(MapTask.java:1586)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.collect(MapTask.java:1099)
	at org.apache.hadoop.mapred.MapTask$NewOutputCollector.write(MapTask.java:727)
	at org.apache.hadoop.mapreduce.task.TaskInputOutputContextImpl.write(TaskInputOutputContextImpl.java:89)
	at org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.write(WrappedMapper.java:112)
	at org.apache.hadoop.mapreduce.Mapper.map(Mapper.java:125)
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:146)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:799)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:347)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:178)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1845)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:172)
Caused by: org.apache.hadoop.util.DiskChecker$DiskErrorException: Could not find any valid local directory for attempt_1612828479085_0009_m_000294_1000_spill_0.out
	at org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.getLocalPathForWrite(LocalDirAllocator.java:462)
	at org.apache.hadoop.fs.LocalDirAllocator.getLocalPathForWrite(LocalDirAllocator.java:165)
	at org.apache.hadoop.fs.LocalDirAllocator.getLocalPathForWrite(LocalDirAllocator.java:146)
	at org.apache.hadoop.mapred.YarnOutputFiles.getSpillFileForWrite(YarnOutputFiles.java:157)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.sortAndSpill(MapTask.java:1617)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.access$1000(MapTask.java:888)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer$SpillThread.run(MapTask.java:1555)

2021-02-09 00:12:33,712 INFO mapreduce.Job: Task Id : attempt_1612828479085_0009_m_000390_1000, Status : FAILED
[2021-02-09 00:12:32.773]/usr/lib/hadoop/tmp/nm-local-dir/usercache/ec2-user/appcache/application_1612828479085_0009/container_1612828479085_0009_02_000131/launch_container.sh (No space left on device)
[2021-02-09 00:12:32.773]/usr/lib/hadoop/tmp/nm-local-dir/usercache/ec2-user/appcache/application_1612828479085_0009/container_1612828479085_0009_02_000131/launch_container.sh (No space left on device)

2021-02-09 00:12:34,726 INFO mapreduce.Job: Task Id : attempt_1612828479085_0009_m_000297_1000, Status : FAILED
Error: java.io.IOException: Spill failed
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.checkSpillException(MapTask.java:1586)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.collect(MapTask.java:1099)
	at org.apache.hadoop.mapred.MapTask$NewOutputCollector.write(MapTask.java:727)
	at org.apache.hadoop.mapreduce.task.TaskInputOutputContextImpl.write(TaskInputOutputContextImpl.java:89)
	at org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.write(WrappedMapper.java:112)
	at org.apache.hadoop.mapreduce.Mapper.map(Mapper.java:125)
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:146)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:799)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:347)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:178)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1845)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:172)
Caused by: org.apache.hadoop.util.DiskChecker$DiskErrorException: Could not find any valid local directory for attempt_1612828479085_0009_m_000297_1000_spill_0.out
	at org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.getLocalPathForWrite(LocalDirAllocator.java:462)
	at org.apache.hadoop.fs.LocalDirAllocator.getLocalPathForWrite(LocalDirAllocator.java:165)
	at org.apache.hadoop.fs.LocalDirAllocator.getLocalPathForWrite(LocalDirAllocator.java:146)
	at org.apache.hadoop.mapred.YarnOutputFiles.getSpillFileForWrite(YarnOutputFiles.java:157)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.sortAndSpill(MapTask.java:1617)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.access$1000(MapTask.java:888)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer$SpillThread.run(MapTask.java:1555)

2021-02-09 00:12:34,745 INFO mapreduce.Job: Task Id : attempt_1612828479085_0009_m_000295_1000, Status : FAILED
Error: java.io.IOException: Spill failed
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.checkSpillException(MapTask.java:1586)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.collect(MapTask.java:1099)
	at org.apache.hadoop.mapred.MapTask$NewOutputCollector.write(MapTask.java:727)
	at org.apache.hadoop.mapreduce.task.TaskInputOutputContextImpl.write(TaskInputOutputContextImpl.java:89)
	at org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.write(WrappedMapper.java:112)
	at org.apache.hadoop.mapreduce.Mapper.map(Mapper.java:125)
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:146)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:799)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:347)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:178)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1845)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:172)
Caused by: org.apache.hadoop.util.DiskChecker$DiskErrorException: Could not find any valid local directory for attempt_1612828479085_0009_m_000295_1000_spill_0.out
	at org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.getLocalPathForWrite(LocalDirAllocator.java:462)
	at org.apache.hadoop.fs.LocalDirAllocator.getLocalPathForWrite(LocalDirAllocator.java:165)
	at org.apache.hadoop.fs.LocalDirAllocator.getLocalPathForWrite(LocalDirAllocator.java:146)
	at org.apache.hadoop.mapred.YarnOutputFiles.getSpillFileForWrite(YarnOutputFiles.java:157)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.sortAndSpill(MapTask.java:1617)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.access$1000(MapTask.java:888)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer$SpillThread.run(MapTask.java:1555)

2021-02-09 00:12:35,783 INFO mapreduce.Job: Task Id : attempt_1612828479085_0009_m_000337_1000, Status : FAILED
Error: java.io.IOException: Spill failed
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.checkSpillException(MapTask.java:1586)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.collect(MapTask.java:1099)
	at org.apache.hadoop.mapred.MapTask$NewOutputCollector.write(MapTask.java:727)
	at org.apache.hadoop.mapreduce.task.TaskInputOutputContextImpl.write(TaskInputOutputContextImpl.java:89)
	at org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.write(WrappedMapper.java:112)
	at org.apache.hadoop.mapreduce.Mapper.map(Mapper.java:125)
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:146)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:799)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:347)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:178)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1845)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:172)
Caused by: org.apache.hadoop.util.DiskChecker$DiskErrorException: Could not find any valid local directory for attempt_1612828479085_0009_m_000337_1000_spill_0.out
	at org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.getLocalPathForWrite(LocalDirAllocator.java:462)
	at org.apache.hadoop.fs.LocalDirAllocator.getLocalPathForWrite(LocalDirAllocator.java:165)
	at org.apache.hadoop.fs.LocalDirAllocator.getLocalPathForWrite(LocalDirAllocator.java:146)
	at org.apache.hadoop.mapred.YarnOutputFiles.getSpillFileForWrite(YarnOutputFiles.java:157)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.sortAndSpill(MapTask.java:1617)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.access$1000(MapTask.java:888)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer$SpillThread.run(MapTask.java:1555)

2021-02-09 00:12:35,784 INFO mapreduce.Job: Task Id : attempt_1612828479085_0009_m_000368_1000, Status : FAILED
FSError: java.io.IOException: No space left on device
2021-02-09 00:12:36,788 INFO mapreduce.Job: Task Id : attempt_1612828479085_0009_m_000339_1000, Status : FAILED
Error: java.io.IOException: Spill failed
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.checkSpillException(MapTask.java:1586)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.collect(MapTask.java:1099)
	at org.apache.hadoop.mapred.MapTask$NewOutputCollector.write(MapTask.java:727)
	at org.apache.hadoop.mapreduce.task.TaskInputOutputContextImpl.write(TaskInputOutputContextImpl.java:89)
	at org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.write(WrappedMapper.java:112)
	at org.apache.hadoop.mapreduce.Mapper.map(Mapper.java:125)
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:146)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:799)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:347)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:178)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1845)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:172)
Caused by: org.apache.hadoop.util.DiskChecker$DiskErrorException: Could not find any valid local directory for attempt_1612828479085_0009_m_000339_1000_spill_0.out
	at org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.getLocalPathForWrite(LocalDirAllocator.java:462)
	at org.apache.hadoop.fs.LocalDirAllocator.getLocalPathForWrite(LocalDirAllocator.java:165)
	at org.apache.hadoop.fs.LocalDirAllocator.getLocalPathForWrite(LocalDirAllocator.java:146)
	at org.apache.hadoop.mapred.YarnOutputFiles.getSpillFileForWrite(YarnOutputFiles.java:157)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.sortAndSpill(MapTask.java:1617)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.access$1000(MapTask.java:888)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer$SpillThread.run(MapTask.java:1555)

2021-02-09 00:12:36,802 INFO mapreduce.Job: Task Id : attempt_1612828479085_0009_m_000422_1000, Status : FAILED
[2021-02-09 00:12:35.685]Could not find any valid local directory for nmPrivate/application_1612828479085_0009/container_1612828479085_0009_02_000139//launch_container.sh

2021-02-09 00:12:37,806 INFO mapreduce.Job: Task Id : attempt_1612828479085_0009_m_000381_1000, Status : FAILED
Error: java.io.FileNotFoundException: /usr/lib/hadoop/tmp/nm-local-dir/usercache/ec2-user/appcache/application_1612828479085_0009/container_1612828479085_0009_02_000128/job.xml (No space left on device)
	at java.io.FileOutputStream.open0(Native Method)
	at java.io.FileOutputStream.open(FileOutputStream.java:270)
	at java.io.FileOutputStream.<init>(FileOutputStream.java:213)
	at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:251)
	at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:234)
	at org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:333)
	at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:322)
	at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:353)
	at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:416)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:479)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:458)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1164)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1144)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1033)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1021)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:697)
	at org.apache.hadoop.mapred.YarnChild.writeLocalJobFile(YarnChild.java:373)
	at org.apache.hadoop.mapred.YarnChild.configureTask(YarnChild.java:355)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:150)

2021-02-09 00:12:37,829 INFO mapreduce.Job: Task Id : attempt_1612828479085_0009_m_000386_1000, Status : FAILED
Error: java.io.FileNotFoundException: /usr/lib/hadoop/tmp/nm-local-dir/usercache/ec2-user/appcache/application_1612828479085_0009/container_1612828479085_0009_02_000129/job.xml (No space left on device)
	at java.io.FileOutputStream.open0(Native Method)
	at java.io.FileOutputStream.open(FileOutputStream.java:270)
	at java.io.FileOutputStream.<init>(FileOutputStream.java:213)
	at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:251)
	at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:234)
	at org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:333)
	at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:322)
	at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:353)
	at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:416)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:479)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:458)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1164)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1144)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1033)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1021)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:697)
	at org.apache.hadoop.mapred.YarnChild.writeLocalJobFile(YarnChild.java:373)
	at org.apache.hadoop.mapred.YarnChild.configureTask(YarnChild.java:355)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:150)

2021-02-09 00:12:37,838 INFO mapreduce.Job: Task Id : attempt_1612828479085_0009_m_000397_1000, Status : FAILED
Error: java.io.FileNotFoundException: /usr/lib/hadoop/tmp/nm-local-dir/usercache/ec2-user/appcache/application_1612828479085_0009/container_1612828479085_0009_02_000132/job.xml (No space left on device)
	at java.io.FileOutputStream.open0(Native Method)
	at java.io.FileOutputStream.open(FileOutputStream.java:270)
	at java.io.FileOutputStream.<init>(FileOutputStream.java:213)
	at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:251)
	at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:234)
	at org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:333)
	at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:322)
	at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:353)
	at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:416)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:479)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:458)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1164)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1144)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1033)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1021)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:697)
	at org.apache.hadoop.mapred.YarnChild.writeLocalJobFile(YarnChild.java:373)
	at org.apache.hadoop.mapred.YarnChild.configureTask(YarnChild.java:355)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:150)

2021-02-09 00:12:37,842 INFO mapreduce.Job: Task Id : attempt_1612828479085_0009_m_000433_1000, Status : FAILED
[2021-02-09 00:12:36.712]/usr/lib/hadoop/tmp/nm-local-dir/usercache/ec2-user/appcache/application_1612828479085_0009/container_1612828479085_0009_02_000142/default_container_executor_session.sh (No space left on device)
[2021-02-09 00:12:36.712]/usr/lib/hadoop/tmp/nm-local-dir/usercache/ec2-user/appcache/application_1612828479085_0009/container_1612828479085_0009_02_000142/default_container_executor_session.sh (No space left on device)

2021-02-09 00:12:37,869 INFO mapreduce.Job: Task Id : attempt_1612828479085_0009_m_000427_1000, Status : FAILED
[2021-02-09 00:12:36.738]/usr/lib/hadoop/tmp/nm-local-dir/usercache/ec2-user/appcache/application_1612828479085_0009/container_1612828479085_0009_02_000140/launch_container.sh (No space left on device)
[2021-02-09 00:12:36.738]/usr/lib/hadoop/tmp/nm-local-dir/usercache/ec2-user/appcache/application_1612828479085_0009/container_1612828479085_0009_02_000140/launch_container.sh (No space left on device)

2021-02-09 00:12:37,869 INFO mapreduce.Job: Task Id : attempt_1612828479085_0009_m_000432_1000, Status : FAILED
[2021-02-09 00:12:36.895]Exception from container-launch.
Container id: container_1612828479085_0009_02_000141
Exit code: 1
Exception message: /usr/lib/hadoop/tmp/nm-local-dir/usercache/ec2-user/appcache/application_1612828479085_0009/container_1612828479085_0009_02_000141/launch_container.sh: line 5: /usr/lib/hadoop/logs/userlogs/application_1612828479085_0009/container_1612828479085_0009_02_000141/prelaunch.out: No space left on device
/usr/lib/hadoop/tmp/nm-local-dir/usercache/ec2-user/appcache/application_1612828479085_0009/container_1612828479085_0009_02_000141/default_container_executor.sh: line 4: /usr/lib/hadoop/tmp/nm-local-dir/nmPrivate/application_1612828479085_0009/container_1612828479085_0009_02_000141/container_1612828479085_0009_02_000141.pid.exitcode.tmp: No space left on device
/bin/mv: cannot stat '/usr/lib/hadoop/tmp/nm-local-dir/nmPrivate/application_1612828479085_0009/container_1612828479085_0009_02_000141/container_1612828479085_0009_02_000141.pid.exitcode.tmp': No such file or directory


[2021-02-09 00:12:36.896]Container exited with a non-zero exit code 1. 
[2021-02-09 00:12:36.900]Container exited with a non-zero exit code 1. 

2021-02-09 00:12:38,908 INFO mapreduce.Job: Task Id : attempt_1612828479085_0009_m_000400_1000, Status : FAILED
FSError: java.io.IOException: No space left on device
2021-02-09 00:12:38,910 INFO mapreduce.Job: Task Id : attempt_1612828479085_0009_m_000409_1000, Status : FAILED
Error: java.io.FileNotFoundException: /usr/lib/hadoop/tmp/nm-local-dir/usercache/ec2-user/appcache/application_1612828479085_0009/container_1612828479085_0009_02_000137/job.xml (No space left on device)
	at java.io.FileOutputStream.open0(Native Method)
	at java.io.FileOutputStream.open(FileOutputStream.java:270)
	at java.io.FileOutputStream.<init>(FileOutputStream.java:213)
	at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:251)
	at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:234)
	at org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:333)
	at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:322)
	at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:353)
	at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:416)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:479)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:458)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1164)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1144)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1033)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1021)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:697)
	at org.apache.hadoop.mapred.YarnChild.writeLocalJobFile(YarnChild.java:373)
	at org.apache.hadoop.mapred.YarnChild.configureTask(YarnChild.java:355)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:150)

2021-02-09 00:12:38,911 INFO mapreduce.Job: Task Id : attempt_1612828479085_0009_m_000398_1000, Status : FAILED
Error: java.io.FileNotFoundException: /usr/lib/hadoop/tmp/nm-local-dir/usercache/ec2-user/appcache/application_1612828479085_0009/container_1612828479085_0009_02_000133/job.xml (No space left on device)
	at java.io.FileOutputStream.open0(Native Method)
	at java.io.FileOutputStream.open(FileOutputStream.java:270)
	at java.io.FileOutputStream.<init>(FileOutputStream.java:213)
	at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:251)
	at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:234)
	at org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:333)
	at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:322)
	at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:353)
	at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:416)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:479)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:458)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1164)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1144)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1033)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1021)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:697)
	at org.apache.hadoop.mapred.YarnChild.writeLocalJobFile(YarnChild.java:373)
	at org.apache.hadoop.mapred.YarnChild.configureTask(YarnChild.java:355)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:150)

2021-02-09 00:12:38,911 INFO mapreduce.Job: Task Id : attempt_1612828479085_0009_m_000350_1000, Status : FAILED
Error: java.io.IOException: Spill failed
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.checkSpillException(MapTask.java:1586)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.collect(MapTask.java:1099)
	at org.apache.hadoop.mapred.MapTask$NewOutputCollector.write(MapTask.java:727)
	at org.apache.hadoop.mapreduce.task.TaskInputOutputContextImpl.write(TaskInputOutputContextImpl.java:89)
	at org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.write(WrappedMapper.java:112)
	at org.apache.hadoop.mapreduce.Mapper.map(Mapper.java:125)
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:146)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:799)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:347)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:178)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1845)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:172)
Caused by: org.apache.hadoop.util.DiskChecker$DiskErrorException: Could not find any valid local directory for attempt_1612828479085_0009_m_000350_1000_spill_0.out
	at org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.getLocalPathForWrite(LocalDirAllocator.java:462)
	at org.apache.hadoop.fs.LocalDirAllocator.getLocalPathForWrite(LocalDirAllocator.java:165)
	at org.apache.hadoop.fs.LocalDirAllocator.getLocalPathForWrite(LocalDirAllocator.java:146)
	at org.apache.hadoop.mapred.YarnOutputFiles.getSpillFileForWrite(YarnOutputFiles.java:157)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.sortAndSpill(MapTask.java:1617)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.access$1000(MapTask.java:888)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer$SpillThread.run(MapTask.java:1555)

2021-02-09 00:12:38,912 INFO mapreduce.Job: Task Id : attempt_1612828479085_0009_m_000359_1000, Status : FAILED
Error: java.io.IOException: Spill failed
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.checkSpillException(MapTask.java:1586)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.collect(MapTask.java:1099)
	at org.apache.hadoop.mapred.MapTask$NewOutputCollector.write(MapTask.java:727)
	at org.apache.hadoop.mapreduce.task.TaskInputOutputContextImpl.write(TaskInputOutputContextImpl.java:89)
	at org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.write(WrappedMapper.java:112)
	at org.apache.hadoop.mapreduce.Mapper.map(Mapper.java:125)
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:146)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:799)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:347)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:178)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1845)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:172)
Caused by: org.apache.hadoop.util.DiskChecker$DiskErrorException: Could not find any valid local directory for attempt_1612828479085_0009_m_000359_1000_spill_0.out
	at org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.getLocalPathForWrite(LocalDirAllocator.java:462)
	at org.apache.hadoop.fs.LocalDirAllocator.getLocalPathForWrite(LocalDirAllocator.java:165)
	at org.apache.hadoop.fs.LocalDirAllocator.getLocalPathForWrite(LocalDirAllocator.java:146)
	at org.apache.hadoop.mapred.YarnOutputFiles.getSpillFileForWrite(YarnOutputFiles.java:157)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.sortAndSpill(MapTask.java:1617)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.access$1000(MapTask.java:888)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer$SpillThread.run(MapTask.java:1555)

2021-02-09 00:12:38,912 INFO mapreduce.Job: Task Id : attempt_1612828479085_0009_m_000378_1000, Status : FAILED
Error: java.io.IOException: Spill failed
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.checkSpillException(MapTask.java:1586)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.collect(MapTask.java:1099)
	at org.apache.hadoop.mapred.MapTask$NewOutputCollector.write(MapTask.java:727)
	at org.apache.hadoop.mapreduce.task.TaskInputOutputContextImpl.write(TaskInputOutputContextImpl.java:89)
	at org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.write(WrappedMapper.java:112)
	at org.apache.hadoop.mapreduce.Mapper.map(Mapper.java:125)
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:146)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:799)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:347)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:178)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1845)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:172)
Caused by: org.apache.hadoop.util.DiskChecker$DiskErrorException: Could not find any valid local directory for attempt_1612828479085_0009_m_000378_1000_spill_0.out
	at org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.getLocalPathForWrite(LocalDirAllocator.java:462)
	at org.apache.hadoop.fs.LocalDirAllocator.getLocalPathForWrite(LocalDirAllocator.java:165)
	at org.apache.hadoop.fs.LocalDirAllocator.getLocalPathForWrite(LocalDirAllocator.java:146)
	at org.apache.hadoop.mapred.YarnOutputFiles.getSpillFileForWrite(YarnOutputFiles.java:157)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.sortAndSpill(MapTask.java:1617)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.access$1000(MapTask.java:888)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer$SpillThread.run(MapTask.java:1555)

2021-02-09 00:12:38,913 INFO mapreduce.Job: Task Id : attempt_1612828479085_0009_m_000414_1000, Status : FAILED
FSError: java.io.IOException: No space left on device
2021-02-09 00:12:38,914 INFO mapreduce.Job: Task Id : attempt_1612828479085_0009_m_000361_1000, Status : FAILED
Error: java.io.IOException: Spill failed
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.checkSpillException(MapTask.java:1586)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.collect(MapTask.java:1099)
	at org.apache.hadoop.mapred.MapTask$NewOutputCollector.write(MapTask.java:727)
	at org.apache.hadoop.mapreduce.task.TaskInputOutputContextImpl.write(TaskInputOutputContextImpl.java:89)
	at org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.write(WrappedMapper.java:112)
	at org.apache.hadoop.mapreduce.Mapper.map(Mapper.java:125)
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:146)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:799)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:347)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:178)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1845)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:172)
Caused by: org.apache.hadoop.util.DiskChecker$DiskErrorException: Could not find any valid local directory for attempt_1612828479085_0009_m_000361_1000_spill_0.out
	at org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.getLocalPathForWrite(LocalDirAllocator.java:462)
	at org.apache.hadoop.fs.LocalDirAllocator.getLocalPathForWrite(LocalDirAllocator.java:165)
	at org.apache.hadoop.fs.LocalDirAllocator.getLocalPathForWrite(LocalDirAllocator.java:146)
	at org.apache.hadoop.mapred.YarnOutputFiles.getSpillFileForWrite(YarnOutputFiles.java:157)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.sortAndSpill(MapTask.java:1617)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.access$1000(MapTask.java:888)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer$SpillThread.run(MapTask.java:1555)

2021-02-09 00:12:38,914 INFO mapreduce.Job: Task Id : attempt_1612828479085_0009_m_000438_1000, Status : FAILED
[2021-02-09 00:12:37.954]Could not find any valid local directory for nmPrivate/application_1612828479085_0009/container_1612828479085_0009_02_000145//launch_container.sh

2021-02-09 00:12:38,915 INFO mapreduce.Job: Task Id : attempt_1612828479085_0009_m_000437_1000, Status : FAILED
[2021-02-09 00:12:37.912]Could not find any valid local directory for nmPrivate/application_1612828479085_0009/container_1612828479085_0009_02_000144//launch_container.sh

2021-02-09 00:12:39,923 INFO mapreduce.Job: Task Id : attempt_1612828479085_0009_m_000446_1000, Status : FAILED
[2021-02-09 00:12:38.023]Could not find any valid local directory for nmPrivate/application_1612828479085_0009/container_1612828479085_0009_02_000147//launch_container.sh

2021-02-09 00:12:39,926 INFO mapreduce.Job: Task Id : attempt_1612828479085_0009_m_000441_1000, Status : FAILED
[2021-02-09 00:12:38.058]Could not find any valid local directory for nmPrivate/application_1612828479085_0009/container_1612828479085_0009_02_000146//launch_container.sh

2021-02-09 00:12:39,936 INFO mapreduce.Job: Task Id : attempt_1612828479085_0009_m_000435_1000, Status : FAILED
[2021-02-09 00:12:37.992]Could not find any valid local directory for nmPrivate/application_1612828479085_0009/container_1612828479085_0009_02_000143//launch_container.sh

2021-02-09 00:12:39,937 INFO mapreduce.Job: Task Id : attempt_1612828479085_0009_m_000448_1000, Status : FAILED
[2021-02-09 00:12:38.071]Could not find any valid local directory for nmPrivate/application_1612828479085_0009/container_1612828479085_0009_02_000148//launch_container.sh

2021-02-09 00:12:39,943 INFO mapreduce.Job: Task Id : attempt_1612828479085_0009_m_000388_1000, Status : FAILED
Error: java.io.IOException: Spill failed
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.checkSpillException(MapTask.java:1586)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.collect(MapTask.java:1099)
	at org.apache.hadoop.mapred.MapTask$NewOutputCollector.write(MapTask.java:727)
	at org.apache.hadoop.mapreduce.task.TaskInputOutputContextImpl.write(TaskInputOutputContextImpl.java:89)
	at org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.write(WrappedMapper.java:112)
	at org.apache.hadoop.mapreduce.Mapper.map(Mapper.java:125)
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:146)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:799)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:347)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:178)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1845)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:172)
Caused by: org.apache.hadoop.util.DiskChecker$DiskErrorException: Could not find any valid local directory for attempt_1612828479085_0009_m_000388_1000_spill_0.out
	at org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.getLocalPathForWrite(LocalDirAllocator.java:462)
	at org.apache.hadoop.fs.LocalDirAllocator.getLocalPathForWrite(LocalDirAllocator.java:165)
	at org.apache.hadoop.fs.LocalDirAllocator.getLocalPathForWrite(LocalDirAllocator.java:146)
	at org.apache.hadoop.mapred.YarnOutputFiles.getSpillFileForWrite(YarnOutputFiles.java:157)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.sortAndSpill(MapTask.java:1617)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.access$1000(MapTask.java:888)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer$SpillThread.run(MapTask.java:1555)

2021-02-09 00:12:39,948 INFO mapreduce.Job: Task Id : attempt_1612828479085_0009_m_000402_1000, Status : FAILED
Error: java.io.IOException: Spill failed
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.checkSpillException(MapTask.java:1586)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.collect(MapTask.java:1099)
	at org.apache.hadoop.mapred.MapTask$NewOutputCollector.write(MapTask.java:727)
	at org.apache.hadoop.mapreduce.task.TaskInputOutputContextImpl.write(TaskInputOutputContextImpl.java:89)
	at org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.write(WrappedMapper.java:112)
	at org.apache.hadoop.mapreduce.Mapper.map(Mapper.java:125)
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:146)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:799)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:347)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:178)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1845)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:172)
Caused by: org.apache.hadoop.util.DiskChecker$DiskErrorException: Could not find any valid local directory for attempt_1612828479085_0009_m_000402_1000_spill_0.out
	at org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.getLocalPathForWrite(LocalDirAllocator.java:462)
	at org.apache.hadoop.fs.LocalDirAllocator.getLocalPathForWrite(LocalDirAllocator.java:165)
	at org.apache.hadoop.fs.LocalDirAllocator.getLocalPathForWrite(LocalDirAllocator.java:146)
	at org.apache.hadoop.mapred.YarnOutputFiles.getSpillFileForWrite(YarnOutputFiles.java:157)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.sortAndSpill(MapTask.java:1617)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.access$1000(MapTask.java:888)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer$SpillThread.run(MapTask.java:1555)

2021-02-09 00:12:39,954 INFO mapreduce.Job: Task Id : attempt_1612828479085_0009_m_000399_1000, Status : FAILED
Error: java.io.IOException: Spill failed
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.checkSpillException(MapTask.java:1586)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.collect(MapTask.java:1099)
	at org.apache.hadoop.mapred.MapTask$NewOutputCollector.write(MapTask.java:727)
	at org.apache.hadoop.mapreduce.task.TaskInputOutputContextImpl.write(TaskInputOutputContextImpl.java:89)
	at org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.write(WrappedMapper.java:112)
	at org.apache.hadoop.mapreduce.Mapper.map(Mapper.java:125)
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:146)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:799)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:347)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:178)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1845)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:172)
Caused by: org.apache.hadoop.util.DiskChecker$DiskErrorException: Could not find any valid local directory for attempt_1612828479085_0009_m_000399_1000_spill_0.out
	at org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.getLocalPathForWrite(LocalDirAllocator.java:462)
	at org.apache.hadoop.fs.LocalDirAllocator.getLocalPathForWrite(LocalDirAllocator.java:165)
	at org.apache.hadoop.fs.LocalDirAllocator.getLocalPathForWrite(LocalDirAllocator.java:146)
	at org.apache.hadoop.mapred.YarnOutputFiles.getSpillFileForWrite(YarnOutputFiles.java:157)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.sortAndSpill(MapTask.java:1617)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.access$1000(MapTask.java:888)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer$SpillThread.run(MapTask.java:1555)

2021-02-09 00:12:39,957 INFO mapreduce.Job: Task Id : attempt_1612828479085_0009_m_000462_1000, Status : FAILED
[2021-02-09 00:12:38.725]Exception from container-launch.
Container id: container_1612828479085_0009_02_000154
Exit code: 1

[2021-02-09 00:12:38.745]Container exited with a non-zero exit code 1. Error file: prelaunch.err.
Last 4096 bytes of prelaunch.err :
ln: failed to create symbolic link 'job.xml': No space left on device

[2021-02-09 00:12:38.752]Container exited with a non-zero exit code 1. Error file: prelaunch.err.
Last 4096 bytes of prelaunch.err :
ln: failed to create symbolic link 'job.xml': No space left on device


2021-02-09 00:12:39,957 INFO mapreduce.Job: Task Id : attempt_1612828479085_0009_m_000452_1000, Status : FAILED
[2021-02-09 00:12:38.793]Exception from container-launch.
Container id: container_1612828479085_0009_02_000150
Exit code: 1

[2021-02-09 00:12:38.800]Container exited with a non-zero exit code 1. Error file: prelaunch.err.
Last 4096 bytes of prelaunch.err :
ln: failed to create symbolic link '_partition.lst': No space left on device

[2021-02-09 00:12:38.829]Container exited with a non-zero exit code 1. Error file: prelaunch.err.
Last 4096 bytes of prelaunch.err :
ln: failed to create symbolic link '_partition.lst': No space left on device


2021-02-09 00:12:47,134 INFO mapreduce.Job:  map 0% reduce 4%
2021-02-09 00:13:09,506 INFO mapreduce.Job:  map 1% reduce 4%
2021-02-09 00:13:10,510 INFO mapreduce.Job:  map 2% reduce 4%
2021-02-09 00:13:11,544 INFO mapreduce.Job:  map 4% reduce 4%
2021-02-09 00:13:16,586 INFO mapreduce.Job:  map 5% reduce 4%
2021-02-09 00:13:17,608 INFO mapreduce.Job:  map 6% reduce 4%
2021-02-09 00:13:29,928 INFO mapreduce.Job:  map 7% reduce 4%
2021-02-09 00:13:40,065 INFO mapreduce.Job:  map 8% reduce 4%
2021-02-09 00:13:48,143 INFO mapreduce.Job:  map 9% reduce 4%
2021-02-09 00:13:53,160 INFO mapreduce.Job:  map 7% reduce 4%
2021-02-09 00:14:06,202 INFO mapreduce.Job:  map 5% reduce 4%
2021-02-09 00:33:51,337 INFO retry.RetryInvocationHandler: java.io.EOFException: End of File Exception between local host is: "hd1/10.0.9.11"; destination host is: "hd1":8032; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException, while invoking ApplicationClientProtocolPBClientImpl.getApplicationReport over null. Trying to failover immediately.
2021-02-09 00:33:52,337 INFO ipc.Client: Retrying connect to server: hd1/10.0.9.11:8032. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2021-02-09 00:33:53,338 INFO ipc.Client: Retrying connect to server: hd1/10.0.9.11:8032. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2021-02-09 00:33:54,338 INFO ipc.Client: Retrying connect to server: hd1/10.0.9.11:8032. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2021-02-09 00:33:55,339 INFO ipc.Client: Retrying connect to server: hd1/10.0.9.11:8032. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2021-02-09 00:33:56,339 INFO ipc.Client: Retrying connect to server: hd1/10.0.9.11:8032. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2021-02-09 00:33:57,340 INFO ipc.Client: Retrying connect to server: hd1/10.0.9.11:8032. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2021-02-09 00:33:58,340 INFO ipc.Client: Retrying connect to server: hd1/10.0.9.11:8032. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2021-02-09 00:33:59,340 INFO ipc.Client: Retrying connect to server: hd1/10.0.9.11:8032. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2021-02-09 00:34:00,341 INFO ipc.Client: Retrying connect to server: hd1/10.0.9.11:8032. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2021-02-09 00:34:01,341 INFO ipc.Client: Retrying connect to server: hd1/10.0.9.11:8032. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2021-02-09 00:34:01,342 INFO retry.RetryInvocationHandler: java.net.ConnectException: Call From hd1/10.0.9.11 to hd1:8032 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking ApplicationClientProtocolPBClientImpl.getApplicationReport over null after 1 failover attempts. Trying to failover after sleeping for 39248ms.
2021-02-09 00:34:41,591 INFO ipc.Client: Retrying connect to server: hd1/10.0.9.11:8032. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2021-02-09 00:34:42,592 INFO ipc.Client: Retrying connect to server: hd1/10.0.9.11:8032. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2021-02-09 00:34:43,592 INFO ipc.Client: Retrying connect to server: hd1/10.0.9.11:8032. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2021-02-09 00:34:44,593 INFO ipc.Client: Retrying connect to server: hd1/10.0.9.11:8032. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2021-02-09 00:34:45,593 INFO ipc.Client: Retrying connect to server: hd1/10.0.9.11:8032. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2021-02-09 00:34:46,594 INFO ipc.Client: Retrying connect to server: hd1/10.0.9.11:8032. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2021-02-09 00:34:47,594 INFO ipc.Client: Retrying connect to server: hd1/10.0.9.11:8032. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2021-02-09 00:34:48,594 INFO ipc.Client: Retrying connect to server: hd1/10.0.9.11:8032. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2021-02-09 00:34:49,595 INFO ipc.Client: Retrying connect to server: hd1/10.0.9.11:8032. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2021-02-09 00:34:50,595 INFO ipc.Client: Retrying connect to server: hd1/10.0.9.11:8032. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2021-02-09 00:34:50,596 INFO retry.RetryInvocationHandler: java.net.ConnectException: Call From hd1/10.0.9.11 to hd1:8032 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking ApplicationClientProtocolPBClientImpl.getApplicationReport over null after 2 failover attempts. Trying to failover after sleeping for 41670ms.
2021-02-09 00:35:33,266 INFO ipc.Client: Retrying connect to server: hd1/10.0.9.11:8032. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2021-02-09 00:35:34,267 INFO ipc.Client: Retrying connect to server: hd1/10.0.9.11:8032. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2021-02-09 00:35:35,267 INFO ipc.Client: Retrying connect to server: hd1/10.0.9.11:8032. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2021-02-09 00:35:36,268 INFO ipc.Client: Retrying connect to server: hd1/10.0.9.11:8032. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2021-02-09 00:35:37,268 INFO ipc.Client: Retrying connect to server: hd1/10.0.9.11:8032. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2021-02-09 00:35:38,268 INFO ipc.Client: Retrying connect to server: hd1/10.0.9.11:8032. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2021-02-09 00:35:39,269 INFO ipc.Client: Retrying connect to server: hd1/10.0.9.11:8032. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2021-02-09 00:35:40,269 INFO ipc.Client: Retrying connect to server: hd1/10.0.9.11:8032. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2021-02-09 00:35:41,270 INFO ipc.Client: Retrying connect to server: hd1/10.0.9.11:8032. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2021-02-09 00:35:42,270 INFO ipc.Client: Retrying connect to server: hd1/10.0.9.11:8032. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2021-02-09 00:35:42,270 INFO retry.RetryInvocationHandler: java.net.ConnectException: Call From hd1/10.0.9.11 to hd1:8032 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking ApplicationClientProtocolPBClientImpl.getApplicationReport over null after 3 failover attempts. Trying to failover after sleeping for 39980ms.
2021-02-09 00:36:23,251 INFO ipc.Client: Retrying connect to server: hd1/10.0.9.11:8032. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2021-02-09 00:36:24,252 INFO ipc.Client: Retrying connect to server: hd1/10.0.9.11:8032. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2021-02-09 00:36:25,252 INFO ipc.Client: Retrying connect to server: hd1/10.0.9.11:8032. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2021-02-09 00:36:26,252 INFO ipc.Client: Retrying connect to server: hd1/10.0.9.11:8032. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2021-02-09 00:36:27,253 INFO ipc.Client: Retrying connect to server: hd1/10.0.9.11:8032. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2021-02-09 00:36:28,253 INFO ipc.Client: Retrying connect to server: hd1/10.0.9.11:8032. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2021-02-09 00:36:29,253 INFO ipc.Client: Retrying connect to server: hd1/10.0.9.11:8032. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2021-02-09 00:36:30,254 INFO ipc.Client: Retrying connect to server: hd1/10.0.9.11:8032. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2021-02-09 00:36:31,254 INFO ipc.Client: Retrying connect to server: hd1/10.0.9.11:8032. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2021-02-09 00:36:32,255 INFO ipc.Client: Retrying connect to server: hd1/10.0.9.11:8032. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2021-02-09 00:36:32,255 INFO retry.RetryInvocationHandler: java.net.ConnectException: Call From hd1/10.0.9.11 to hd1:8032 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking ApplicationClientProtocolPBClientImpl.getApplicationReport over null after 4 failover attempts. Trying to failover after sleeping for 44849ms.
2021-02-09 00:37:18,105 INFO ipc.Client: Retrying connect to server: hd1/10.0.9.11:8032. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2021-02-09 00:37:19,105 INFO ipc.Client: Retrying connect to server: hd1/10.0.9.11:8032. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2021-02-09 00:37:20,106 INFO ipc.Client: Retrying connect to server: hd1/10.0.9.11:8032. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2021-02-09 00:37:21,106 INFO ipc.Client: Retrying connect to server: hd1/10.0.9.11:8032. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2021-02-09 00:37:22,107 INFO ipc.Client: Retrying connect to server: hd1/10.0.9.11:8032. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2021-02-09 00:37:23,107 INFO ipc.Client: Retrying connect to server: hd1/10.0.9.11:8032. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2021-02-09 00:37:24,107 INFO ipc.Client: Retrying connect to server: hd1/10.0.9.11:8032. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2021-02-09 00:37:25,108 INFO ipc.Client: Retrying connect to server: hd1/10.0.9.11:8032. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2021-02-09 00:37:26,108 INFO ipc.Client: Retrying connect to server: hd1/10.0.9.11:8032. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2021-02-09 00:37:27,108 INFO ipc.Client: Retrying connect to server: hd1/10.0.9.11:8032. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2021-02-09 00:37:27,109 INFO retry.RetryInvocationHandler: java.net.ConnectException: Call From hd1/10.0.9.11 to hd1:8032 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking ApplicationClientProtocolPBClientImpl.getApplicationReport over null after 5 failover attempts. Trying to failover after sleeping for 35207ms.
2021-02-09 00:38:03,317 INFO ipc.Client: Retrying connect to server: hd1/10.0.9.11:8032. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2021-02-09 00:38:04,317 INFO ipc.Client: Retrying connect to server: hd1/10.0.9.11:8032. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2021-02-09 00:38:05,318 INFO ipc.Client: Retrying connect to server: hd1/10.0.9.11:8032. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2021-02-09 00:38:06,318 INFO ipc.Client: Retrying connect to server: hd1/10.0.9.11:8032. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2021-02-09 00:38:07,318 INFO ipc.Client: Retrying connect to server: hd1/10.0.9.11:8032. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2021-02-09 00:38:08,319 INFO ipc.Client: Retrying connect to server: hd1/10.0.9.11:8032. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2021-02-09 00:38:09,319 INFO ipc.Client: Retrying connect to server: hd1/10.0.9.11:8032. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2021-02-09 00:38:10,319 INFO ipc.Client: Retrying connect to server: hd1/10.0.9.11:8032. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2021-02-09 00:38:11,320 INFO ipc.Client: Retrying connect to server: hd1/10.0.9.11:8032. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2021-02-09 00:38:12,320 INFO ipc.Client: Retrying connect to server: hd1/10.0.9.11:8032. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2021-02-09 00:38:12,321 INFO retry.RetryInvocationHandler: java.net.ConnectException: Call From hd1/10.0.9.11 to hd1:8032 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking ApplicationClientProtocolPBClientImpl.getApplicationReport over null after 6 failover attempts. Trying to failover after sleeping for 23089ms.
2021-02-09 00:38:36,411 INFO ipc.Client: Retrying connect to server: hd1/10.0.9.11:8032. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2021-02-09 00:38:37,411 INFO ipc.Client: Retrying connect to server: hd1/10.0.9.11:8032. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2021-02-09 00:38:38,411 INFO ipc.Client: Retrying connect to server: hd1/10.0.9.11:8032. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2021-02-09 00:38:39,412 INFO ipc.Client: Retrying connect to server: hd1/10.0.9.11:8032. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2021-02-09 00:38:40,412 INFO ipc.Client: Retrying connect to server: hd1/10.0.9.11:8032. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2021-02-09 00:38:41,412 INFO ipc.Client: Retrying connect to server: hd1/10.0.9.11:8032. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2021-02-09 00:38:42,413 INFO ipc.Client: Retrying connect to server: hd1/10.0.9.11:8032. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2021-02-09 00:38:43,413 INFO ipc.Client: Retrying connect to server: hd1/10.0.9.11:8032. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2021-02-09 00:38:44,414 INFO ipc.Client: Retrying connect to server: hd1/10.0.9.11:8032. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2021-02-09 00:38:45,414 INFO ipc.Client: Retrying connect to server: hd1/10.0.9.11:8032. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2021-02-09 00:38:45,415 INFO retry.RetryInvocationHandler: java.net.ConnectException: Call From hd1/10.0.9.11 to hd1:8032 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking ApplicationClientProtocolPBClientImpl.getApplicationReport over null after 7 failover attempts. Trying to failover after sleeping for 25183ms.
2021-02-09 00:39:11,598 INFO ipc.Client: Retrying connect to server: hd1/10.0.9.11:8032. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2021-02-09 00:39:12,599 INFO ipc.Client: Retrying connect to server: hd1/10.0.9.11:8032. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2021-02-09 00:39:13,599 INFO ipc.Client: Retrying connect to server: hd1/10.0.9.11:8032. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2021-02-09 00:39:14,599 INFO ipc.Client: Retrying connect to server: hd1/10.0.9.11:8032. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2021-02-09 00:39:15,600 INFO ipc.Client: Retrying connect to server: hd1/10.0.9.11:8032. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2021-02-09 00:39:16,600 INFO ipc.Client: Retrying connect to server: hd1/10.0.9.11:8032. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2021-02-09 00:39:17,601 INFO ipc.Client: Retrying connect to server: hd1/10.0.9.11:8032. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2021-02-09 00:39:18,601 INFO ipc.Client: Retrying connect to server: hd1/10.0.9.11:8032. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2021-02-09 00:39:19,601 INFO ipc.Client: Retrying connect to server: hd1/10.0.9.11:8032. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2021-02-09 00:39:20,602 INFO ipc.Client: Retrying connect to server: hd1/10.0.9.11:8032. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2021-02-09 00:39:20,603 INFO retry.RetryInvocationHandler: java.net.ConnectException: Call From hd1/10.0.9.11 to hd1:8032 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking ApplicationClientProtocolPBClientImpl.getApplicationReport over null after 8 failover attempts. Trying to failover after sleeping for 19722ms.
2021-02-09 00:39:41,325 INFO ipc.Client: Retrying connect to server: hd1/10.0.9.11:8032. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2021-02-09 00:39:42,326 INFO ipc.Client: Retrying connect to server: hd1/10.0.9.11:8032. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2021-02-09 00:39:43,326 INFO ipc.Client: Retrying connect to server: hd1/10.0.9.11:8032. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2021-02-09 00:39:44,326 INFO ipc.Client: Retrying connect to server: hd1/10.0.9.11:8032. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2021-02-09 00:39:45,327 INFO ipc.Client: Retrying connect to server: hd1/10.0.9.11:8032. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2021-02-09 00:39:46,327 INFO ipc.Client: Retrying connect to server: hd1/10.0.9.11:8032. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2021-02-09 00:39:47,327 INFO ipc.Client: Retrying connect to server: hd1/10.0.9.11:8032. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2021-02-09 00:39:48,328 INFO ipc.Client: Retrying connect to server: hd1/10.0.9.11:8032. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2021-02-09 00:39:49,328 INFO ipc.Client: Retrying connect to server: hd1/10.0.9.11:8032. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2021-02-09 00:39:50,329 INFO ipc.Client: Retrying connect to server: hd1/10.0.9.11:8032. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2021-02-09 00:39:50,329 INFO retry.RetryInvocationHandler: java.net.ConnectException: Call From hd1/10.0.9.11 to hd1:8032 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking ApplicationClientProtocolPBClientImpl.getApplicationReport over null after 9 failover attempts. Trying to failover after sleeping for 17334ms.
2021-02-09 00:40:08,664 INFO ipc.Client: Retrying connect to server: hd1/10.0.9.11:8032. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2021-02-09 00:40:09,664 INFO ipc.Client: Retrying connect to server: hd1/10.0.9.11:8032. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2021-02-09 00:40:10,665 INFO ipc.Client: Retrying connect to server: hd1/10.0.9.11:8032. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2021-02-09 00:40:11,665 INFO ipc.Client: Retrying connect to server: hd1/10.0.9.11:8032. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2021-02-09 00:40:12,666 INFO ipc.Client: Retrying connect to server: hd1/10.0.9.11:8032. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2021-02-09 00:40:13,666 INFO ipc.Client: Retrying connect to server: hd1/10.0.9.11:8032. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2021-02-09 00:40:14,666 INFO ipc.Client: Retrying connect to server: hd1/10.0.9.11:8032. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2021-02-09 00:40:15,667 INFO ipc.Client: Retrying connect to server: hd1/10.0.9.11:8032. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2021-02-09 00:40:16,667 INFO ipc.Client: Retrying connect to server: hd1/10.0.9.11:8032. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2021-02-09 00:40:17,668 INFO ipc.Client: Retrying connect to server: hd1/10.0.9.11:8032. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2021-02-09 00:40:17,668 INFO retry.RetryInvocationHandler: java.net.ConnectException: Call From hd1/10.0.9.11 to hd1:8032 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking ApplicationClientProtocolPBClientImpl.getApplicationReport over null after 10 failover attempts. Trying to failover after sleeping for 34172ms.
2021-02-09 00:40:52,841 INFO ipc.Client: Retrying connect to server: hd1/10.0.9.11:8032. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2021-02-09 00:40:53,842 INFO ipc.Client: Retrying connect to server: hd1/10.0.9.11:8032. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2021-02-09 00:40:54,842 INFO ipc.Client: Retrying connect to server: hd1/10.0.9.11:8032. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2021-02-09 00:40:55,842 INFO ipc.Client: Retrying connect to server: hd1/10.0.9.11:8032. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2021-02-09 00:40:56,843 INFO ipc.Client: Retrying connect to server: hd1/10.0.9.11:8032. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2021-02-09 00:40:57,843 INFO ipc.Client: Retrying connect to server: hd1/10.0.9.11:8032. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2021-02-09 00:40:58,843 INFO ipc.Client: Retrying connect to server: hd1/10.0.9.11:8032. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2021-02-09 00:40:59,844 INFO ipc.Client: Retrying connect to server: hd1/10.0.9.11:8032. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2021-02-09 00:41:00,844 INFO ipc.Client: Retrying connect to server: hd1/10.0.9.11:8032. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2021-02-09 00:41:01,845 INFO ipc.Client: Retrying connect to server: hd1/10.0.9.11:8032. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2021-02-09 00:41:01,845 INFO retry.RetryInvocationHandler: java.net.ConnectException: Call From hd1/10.0.9.11 to hd1:8032 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking ApplicationClientProtocolPBClientImpl.getApplicationReport over null after 11 failover attempts. Trying to failover after sleeping for 44735ms.
2021-02-09 00:41:47,581 INFO ipc.Client: Retrying connect to server: hd1/10.0.9.11:8032. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2021-02-09 00:41:48,582 INFO ipc.Client: Retrying connect to server: hd1/10.0.9.11:8032. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2021-02-09 00:41:49,582 INFO ipc.Client: Retrying connect to server: hd1/10.0.9.11:8032. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2021-02-09 00:41:50,582 INFO ipc.Client: Retrying connect to server: hd1/10.0.9.11:8032. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2021-02-09 00:41:51,583 INFO ipc.Client: Retrying connect to server: hd1/10.0.9.11:8032. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2021-02-09 00:41:52,583 INFO ipc.Client: Retrying connect to server: hd1/10.0.9.11:8032. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2021-02-09 00:41:53,584 INFO ipc.Client: Retrying connect to server: hd1/10.0.9.11:8032. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2021-02-09 00:41:54,584 INFO ipc.Client: Retrying connect to server: hd1/10.0.9.11:8032. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2021-02-09 00:41:55,585 INFO ipc.Client: Retrying connect to server: hd1/10.0.9.11:8032. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2021-02-09 00:41:56,585 INFO ipc.Client: Retrying connect to server: hd1/10.0.9.11:8032. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2021-02-09 00:41:56,586 INFO retry.RetryInvocationHandler: java.net.ConnectException: Call From hd1/10.0.9.11 to hd1:8032 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking ApplicationClientProtocolPBClientImpl.getApplicationReport over null after 12 failover attempts. Trying to failover after sleeping for 23753ms.
2021-02-09 00:42:21,339 INFO ipc.Client: Retrying connect to server: hd1/10.0.9.11:8032. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2021-02-09 00:42:22,340 INFO ipc.Client: Retrying connect to server: hd1/10.0.9.11:8032. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2021-02-09 00:42:23,340 INFO ipc.Client: Retrying connect to server: hd1/10.0.9.11:8032. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2021-02-09 00:42:24,340 INFO ipc.Client: Retrying connect to server: hd1/10.0.9.11:8032. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2021-02-09 00:42:25,341 INFO ipc.Client: Retrying connect to server: hd1/10.0.9.11:8032. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2021-02-09 00:42:26,341 INFO ipc.Client: Retrying connect to server: hd1/10.0.9.11:8032. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2021-02-09 00:42:27,342 INFO ipc.Client: Retrying connect to server: hd1/10.0.9.11:8032. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2021-02-09 00:42:28,342 INFO ipc.Client: Retrying connect to server: hd1/10.0.9.11:8032. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2021-02-09 00:42:29,343 INFO ipc.Client: Retrying connect to server: hd1/10.0.9.11:8032. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2021-02-09 00:42:30,343 INFO ipc.Client: Retrying connect to server: hd1/10.0.9.11:8032. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2021-02-09 00:42:30,344 INFO retry.RetryInvocationHandler: java.net.ConnectException: Call From hd1/10.0.9.11 to hd1:8032 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking ApplicationClientProtocolPBClientImpl.getApplicationReport over null after 13 failover attempts. Trying to failover after sleeping for 16176ms.
2021-02-09 00:42:47,521 INFO ipc.Client: Retrying connect to server: hd1/10.0.9.11:8032. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2021-02-09 00:42:48,521 INFO ipc.Client: Retrying connect to server: hd1/10.0.9.11:8032. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2021-02-09 00:42:49,521 INFO ipc.Client: Retrying connect to server: hd1/10.0.9.11:8032. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2021-02-09 00:42:50,522 INFO ipc.Client: Retrying connect to server: hd1/10.0.9.11:8032. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2021-02-09 00:42:51,522 INFO ipc.Client: Retrying connect to server: hd1/10.0.9.11:8032. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2021-02-09 00:42:52,523 INFO ipc.Client: Retrying connect to server: hd1/10.0.9.11:8032. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2021-02-09 00:42:53,523 INFO ipc.Client: Retrying connect to server: hd1/10.0.9.11:8032. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2021-02-09 00:42:54,523 INFO ipc.Client: Retrying connect to server: hd1/10.0.9.11:8032. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2021-02-09 00:42:55,524 INFO ipc.Client: Retrying connect to server: hd1/10.0.9.11:8032. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2021-02-09 00:42:56,524 INFO ipc.Client: Retrying connect to server: hd1/10.0.9.11:8032. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2021-02-09 00:42:56,525 INFO retry.RetryInvocationHandler: java.net.ConnectException: Call From hd1/10.0.9.11 to hd1:8032 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking ApplicationClientProtocolPBClientImpl.getApplicationReport over null after 14 failover attempts. Trying to failover after sleeping for 30771ms.
2021-02-09 00:43:28,297 INFO ipc.Client: Retrying connect to server: hd1/10.0.9.11:8032. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2021-02-09 00:43:29,297 INFO ipc.Client: Retrying connect to server: hd1/10.0.9.11:8032. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2021-02-09 00:43:30,297 INFO ipc.Client: Retrying connect to server: hd1/10.0.9.11:8032. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2021-02-09 00:43:31,298 INFO ipc.Client: Retrying connect to server: hd1/10.0.9.11:8032. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2021-02-09 00:43:32,298 INFO ipc.Client: Retrying connect to server: hd1/10.0.9.11:8032. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2021-02-09 00:43:33,299 INFO ipc.Client: Retrying connect to server: hd1/10.0.9.11:8032. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2021-02-09 00:43:34,299 INFO ipc.Client: Retrying connect to server: hd1/10.0.9.11:8032. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2021-02-09 00:43:35,299 INFO ipc.Client: Retrying connect to server: hd1/10.0.9.11:8032. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2021-02-09 00:43:36,300 INFO ipc.Client: Retrying connect to server: hd1/10.0.9.11:8032. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2021-02-09 00:43:37,300 INFO ipc.Client: Retrying connect to server: hd1/10.0.9.11:8032. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2021-02-09 00:43:37,301 INFO retry.RetryInvocationHandler: java.net.ConnectException: Call From hd1/10.0.9.11 to hd1:8032 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking ApplicationClientProtocolPBClientImpl.getApplicationReport over null after 15 failover attempts. Trying to failover after sleeping for 43461ms.
[0;31m======== HSsort Result FAILURE ========[0m

[0;32mListing HSsort output [0m




[0;32mStarting HSValidate [0m

2021-02-09 00:43:46,118 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at hd1/10.0.9.11:8032
Exception in thread "main" java.net.ConnectException: Call From hd1/10.0.9.11 to hd1:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:837)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:757)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1566)
	at org.apache.hadoop.ipc.Client.call(Client.java:1508)
	at org.apache.hadoop.ipc.Client.call(Client.java:1405)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:234)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:119)
	at com.sun.proxy.$Proxy9.getFileInfo(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo(ClientNamenodeProtocolTranslatorPB.java:964)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
	at com.sun.proxy.$Proxy10.getFileInfo(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:1731)
	at org.apache.hadoop.hdfs.DistributedFileSystem$29.doCall(DistributedFileSystem.java:1725)
	at org.apache.hadoop.hdfs.DistributedFileSystem$29.doCall(DistributedFileSystem.java:1722)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1737)
	at org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1729)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:163)
	at org.apache.hadoop.mapreduce.JobSubmitter.checkSpecs(JobSubmitter.java:277)
	at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:143)
	at org.apache.hadoop.mapreduce.Job$11.run(Job.java:1576)
	at org.apache.hadoop.mapreduce.Job$11.run(Job.java:1573)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1845)
	at org.apache.hadoop.mapreduce.Job.submit(Job.java:1573)
	at org.apache.hadoop.mapreduce.Job.waitForCompletion(Job.java:1594)
	at HSValidate.run(HSValidate.java:176)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:76)
	at HSValidate.main(HSValidate.java:183)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.util.RunJar.run(RunJar.java:323)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:236)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:533)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:699)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:812)
	at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:413)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1636)
	at org.apache.hadoop.ipc.Client.call(Client.java:1452)
	... 40 more

real	0m1.673s
user	0m2.745s
sys	0m0.050s
[0;31m======== HSsort Result FAILURE ========[0m

[0;32mListing HSValidate output [0m




[0;31m===================================[0m
[0;31mNo Performance Metric (HSph@SF) as some tests Failed [0m
[0;31m===================================[0m
[0;32m===================================[0m
[0;32mDeleting Previous Data - Start - Tue Feb  9 00:43:48 UTC 2021[0m
[0;32mDeleting Previous Data - End - Tue Feb  9 00:44:50 UTC 2021[0m
[0;32m===================================[0m


[0;32m===================================[0m
[0;32m Running BigData TPCx-HS Benchmark Suite (MapReduce) - Run 2 - Epoch 1612831490 [0m
[0;32m TPCx-HS Version 2.0.3 [0m
[0;32m===================================[0m

[0;32mStarting HSGen Run 2 (output being written to ./logs/HSgen-time-run2.txt)[0m

Exception in thread "main" java.net.ConnectException: Call From hd1/10.0.9.11 to hd1:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:837)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:757)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1566)
	at org.apache.hadoop.ipc.Client.call(Client.java:1508)
	at org.apache.hadoop.ipc.Client.call(Client.java:1405)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:234)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:119)
	at com.sun.proxy.$Proxy9.getFileInfo(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo(ClientNamenodeProtocolTranslatorPB.java:964)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
	at com.sun.proxy.$Proxy10.getFileInfo(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:1731)
	at org.apache.hadoop.hdfs.DistributedFileSystem$29.doCall(DistributedFileSystem.java:1725)
	at org.apache.hadoop.hdfs.DistributedFileSystem$29.doCall(DistributedFileSystem.java:1722)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1737)
	at org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1729)
	at HSGen.run(HSGen.java:292)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:76)
	at HSGen.main(HSGen.java:309)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.util.RunJar.run(RunJar.java:323)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:236)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:533)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:699)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:812)
	at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:413)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1636)
	at org.apache.hadoop.ipc.Client.call(Client.java:1452)
	... 30 more

real	0m1.425s
user	0m2.352s
sys	0m0.039s
[0;31m======== HSgen Result FAILURE ========[0m

[0;32mListing HSGen output [0m




[0;32mStarting HSSort Run 2 (output being written to ./logs/HSsort-time-run2.txt)[0m

2021-02-09 00:44:54,312 INFO HSSort: starting
2021-02-09 00:44:54,955 ERROR HSSort: Call From hd1/10.0.9.11 to hd1:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused

real	0m1.433s
user	0m2.298s
sys	0m0.072s
[0;31m======== HSsort Result FAILURE ========[0m

[0;32mListing HSsort output [0m




[0;32mStarting HSValidate [0m

2021-02-09 00:44:57,847 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at hd1/10.0.9.11:8032
Exception in thread "main" java.net.ConnectException: Call From hd1/10.0.9.11 to hd1:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:837)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:757)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1566)
	at org.apache.hadoop.ipc.Client.call(Client.java:1508)
	at org.apache.hadoop.ipc.Client.call(Client.java:1405)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:234)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:119)
	at com.sun.proxy.$Proxy9.getFileInfo(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo(ClientNamenodeProtocolTranslatorPB.java:964)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
	at com.sun.proxy.$Proxy10.getFileInfo(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:1731)
	at org.apache.hadoop.hdfs.DistributedFileSystem$29.doCall(DistributedFileSystem.java:1725)
	at org.apache.hadoop.hdfs.DistributedFileSystem$29.doCall(DistributedFileSystem.java:1722)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1737)
	at org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1729)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:163)
	at org.apache.hadoop.mapreduce.JobSubmitter.checkSpecs(JobSubmitter.java:277)
	at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:143)
	at org.apache.hadoop.mapreduce.Job$11.run(Job.java:1576)
	at org.apache.hadoop.mapreduce.Job$11.run(Job.java:1573)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1845)
	at org.apache.hadoop.mapreduce.Job.submit(Job.java:1573)
	at org.apache.hadoop.mapreduce.Job.waitForCompletion(Job.java:1594)
	at HSValidate.run(HSValidate.java:176)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:76)
	at HSValidate.main(HSValidate.java:183)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.util.RunJar.run(RunJar.java:323)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:236)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:533)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:699)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:812)
	at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:413)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1636)
	at org.apache.hadoop.ipc.Client.call(Client.java:1452)
	... 40 more

real	0m1.636s
user	0m2.741s
sys	0m0.061s
[0;31m======== HSsort Result FAILURE ========[0m

[0;32mListing HSValidate output [0m




[0;31m===================================[0m
[0;31mNo Performance Metric (HSph@SF) as some tests Failed [0m
[0;31m===================================[0m
